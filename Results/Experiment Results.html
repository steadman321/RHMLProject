<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Experiment Results</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="bfe17d38-ab1d-4eac-9b11-2e4ffb327882" class="page sans"><header><h1 class="page-title">Experiment Results</h1></header><div class="page-body"><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="dc17e023-de3c-4ed3-acd4-16ccf99e3101"><div style="font-size:1.5em"><span class="icon">ℹ️</span></div><div style="width:100%">A supporting document for the Ensemble Methods dissertation</div></figure><p id="7cf8ab63-d54d-4b2b-a53c-9b168a41c23c" class="">
</p><p id="954843ca-22ea-48e1-a6cb-1bba74269b69" class=""><span style="border-bottom:0.05em solid"><strong>Contents: </strong></span></p><nav id="787fb702-4a33-49bd-80a2-02691c3fcaa8" class="block-color-blue table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#7fc41261-218d-43bf-9490-3e6c9072853a">1. Overview</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#3209ac47-ab01-4b54-936b-2bb051e4cac6">1.1 Datasets</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#440d6424-c84a-4c99-9e2f-000fa0e4fb0a">1.2 <strong>Running tests : </strong></a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#dd08955b-186b-4b19-b2ec-ea74daf9057c">2. Classification - Wine Dataset</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#71a61b3a-e69a-4068-a7ec-35dd402b2e3e">2.1 BaseLine Decision Tree</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#f9025fe8-801f-4b1d-82ff-466043835884">2.2 Ensemble 1 - Bagging</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c81ffaa4-4978-4233-b9a3-328547cd0415">2.3 Ensemble 2 - Random Forest</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#cfc0d4df-7b6c-40d8-a002-42b9801d5eaa">2.4 Ensemble 3 - Boosting (AdaBoost)</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#281eb5a1-f216-47cd-b109-bcf9e5497e4d">3. Classification - Cars Dataset</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#ca38d7f1-49ed-4c0e-ab96-c27561bf7804">3.1 BaseLine Decision Tree</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#e36f9c52-a765-49dc-b07f-b056e005bee7">3.2 Ensemble 1 - Bagging</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#fa48c8a1-d45c-4bce-a3a2-53dcc267611c">3.3 Ensemble 2 - Random Forest</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#092553e0-6eca-4e81-8d96-3fe35195e4eb">3.4 Ensemble 3 - Boosting (AdaBoost)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1dc2f631-26e3-4852-830e-605804aaf223">3.5 Ensemble 3a - Boosting (AdaBoost - amended)</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#48f9d725-5512-4197-8e8c-66a74b969877">4. Regression - Housing Dataset</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1e43f78c-c264-441c-a47b-658f9d265b6f">4.1 BaseLine Decision Tree</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#cd23ba42-a105-4493-8eeb-d414c6c16cd2">4.2 Ensemble 1 - Bagging </a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d5603143-b3c7-4c72-8b63-dbff9fd1186a">4.3 Ensemble 2 - Random Forest</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#ff722bc7-9f46-4c25-8c8f-5117d9208fd8">4.4 Ensemble 3 - Boosting (Gradient Boosting)</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#17c3f931-2d9a-4a63-a100-48f56da03ac1">5. Regression - Concrete Dataset</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#6447c3bb-57ab-4a3b-a1bf-c26767a5c96b">5.1 BaseLine Decision Tree</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#6a6765a5-1fd6-4526-a77d-197721bff1d4">5.2 Ensemble 1 - Bagging </a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#a372f901-c511-4f04-9a27-ab41d39d06a9">5.3 Ensemble 2 - Random Forest</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#642416c7-e06f-4fc1-b684-dae92c5b1069">5.4 Ensemble 3 - Boosting (Gradient Boosting)</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#f9632e1d-acf5-4db4-983e-018e0d3d8f05">6. Cross Model Analysis </a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#90f73491-1bfe-45fe-a3e6-5e3c62a55a59">6.1 How does &#x27;num_trees&#x27; impact model performance?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#3149dca9-8063-49f5-b343-0bbf1da7b5a8">6.2 How does &#x27;max_depth&#x27; impact model performance?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c0662cf9-4d63-454c-95b8-26d5c578019d">6.3 how does &#x27;min_split&#x27; impact model performance?</a></div></nav><p id="81c33f1e-1589-4b07-9c6d-fc6132d3201a" class="">
</p><hr id="fc1a5093-b95f-408c-a593-9b446868dae8"/><h1 id="7fc41261-218d-43bf-9490-3e6c9072853a" class="">1. Overview</h1><p id="09211a52-04e4-4066-9ff8-031f912e702d" class="">These notes track the use of the RHML/RHML_CMD tools to perform various experiments exploring the use of ensemble models. </p><p id="0005a4aa-dc4c-4d10-ae1a-ba79abe64325" class="">The full source code for RHML/RHML_CMD is available , along with this report, here : <a href="https://github.com/steadman321/RHMLProject">RHMLProject</a></p><p id="a5350a06-c653-49d5-918c-6cc98be6e7cb" class="">The source also contains all the config files required to repeat the tests (see RHML_CMD/Config).</p><p id="34fc7ea3-d0cc-4272-aadd-1174b0d2fd15" class="">If any of the detailed results reports are difficult to read in your browser, double click to get a better view, or go to the source and look at that report from there (see RHML_CMD/reports)</p><p id="cb89ac6e-7881-40c8-8297-ee1c58e3c638" class="">
</p><p id="df6eccf0-c608-4af2-ba0b-453586f298d1" class=""><strong>TOGGLE</strong> <strong>TIP</strong>: a lot of the sections below are in toggle sections, and it can get quite &#x27;busy&#x27; with lots of them open. To &#x27;reset&#x27; you can quickly close/open all toggle sections with : Cmd + Option + T (Mac), Ctrl+Alt+T(Windows). Alternatively, select a number of sections with toggles, click one and all toggles in the selection will open/close.</p><h2 id="3209ac47-ab01-4b54-936b-2bb051e4cac6" class="">1.1 Datasets</h2><p id="cdec9c42-54cb-4dc9-9d37-772b6bb2f8b2" class="">The following datasets from the UCI repository are used during the experiments :</p><p id="207fd353-70c7-4c5c-a335-2328a1826d54" class="">Classification : <a href="https://archive.ics.uci.edu/ml/datasets/Wine">Wine</a> , <a href="https://archive.ics.uci.edu/ml/datasets/car+evaluation">Cars</a></p><p id="2ce289dc-6312-4bf0-8675-a98716ae1f1a" class="">Regression : <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/">Housing</a> , <a href="https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength">Concrete</a></p><h2 id="440d6424-c84a-4c99-9e2f-000fa0e4fb0a" class="">1.2 <strong>Running tests : </strong></h2><p id="d3c84325-d934-4d34-9409-8e8c82701aee" class=""><strong>(See USER GUIDE for full instructions)</strong></p><p id="a7d6f552-99aa-4558-8e25-103d08862809" class="">To run any of the tests mentioned here use the pre-canned config files found in the source: </p><pre id="b8b69158-60b7-4cea-ae73-e467a2e2a946" class="code"><code>#Change to the dir that has got RHML_CMD
cd /Users/&lt;username&gt;/.../RHML_CMD

#Pick a config file to run, for example for Wine test 1, use :
python ./rhml_cmd.py -config=./config/classification/wine/test1.ini</code></pre><h1 id="dd08955b-186b-4b19-b2ec-ea74daf9057c" class="">2. Classification - Wine Dataset</h1><h2 id="71a61b3a-e69a-4068-a7ec-35dd402b2e3e" class="">2.1 BaseLine Decision Tree</h2><p id="f7931fef-cb7f-4c14-a2bf-c5623ea1ef35" class="">The first experiment is to establish a baseline; what can a single base model achieve? All of the base models used in the ensembles are decision trees. Therefore we first create a single decision tree and see what happens when we give it the wine dataset. </p><ul id="1c179436-0646-4eea-9384-681932a8694b" class="toggle"><li><details><summary>Test 1 - baseline with defaults </summary><ul id="8bdc90b3-6d9b-4076-a728-da3046e24e25" class="toggle"><li><details><summary><strong>Config  : ./config/classification/wine/test1.ini</strong></summary><p id="c5205fee-e130-4a63-be50-6fd63dcb4658" class="">Minimal needed: just the dataset to use and the model type to run</p><figure id="42655887-eab2-4a72-a41f-af613de24524" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_15.57.59.png"><img style="width:510px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_15.57.59.png"/></a></figure></details></li></ul><ul id="065ba476-2d87-4b71-9881-7542d57f00c8" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="088283e6-0475-4432-84cb-e95f2d12c98e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_16.02.24.png"><img style="width:1152px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_16.02.24.png"/></a></figure></details></li></ul><ul id="d1009a71-9ef6-41b0-a974-0a0b1543ebc5" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="ba17b9a3-8fb0-4cd1-a1db-d56afb90370f" class="">The default decision tree, grown fully with no pruning easily manages to overfit the data, showing 100% prediction performance on the training data, but only 93.22 on the test data.</p><p id="ee01ade0-5f0b-46ed-91e0-b0d832adee9d" class="">Despite having 13 features it only used 4 of these features.</p><p id="de4e774e-84d3-404f-9b41-8be1e44f0d58" class="">No hyperparamters were set, but in the settings in the report we see :</p><ul id="510b4b00-f6ba-451e-b677-21328e9234c5" class="bulleted-list"><li>min_split = 25</li></ul><ul id="e57bae1b-8822-4dd9-8fb6-2928f3fdf7c5" class="bulleted-list"><li>max_depth = 1000</li></ul><p id="ee185134-1825-4cf5-8845-5fa16074ee2e" class="">These are code defaults; i.e when not set in config these fall back to the hard coded defaults for the model.</p><p id="3d9919c8-922c-41a1-928b-1f3c38910c60" class="">There are not many hyperparams we can tune for our base model. The only 2 parameters are as above. </p><p id="d97ecdb9-07c2-43f3-ad6a-96e6239cf078" class="">Note, that although max depth seems large, and min_split is 25 , we ended up with a tree that only has 3 levels (with a total of 5 terminal nodes) and some of those have more data than the minimum split level. The reason these did not split will be because these are all of the same class already so splitting makes no improvement</p><p id="0e2b6f14-f417-443a-aab5-7c7decc1a5ee" class="">What happens for our default classification tree if we amend the min_split and max_depth values? The next few tests will address this with grid search tests.</p></details></li></ul></details></li></ul><ul id="0351e29d-f650-4ef5-a227-ea21227a45b6" class="toggle"><li><details><summary>Test 2 - grid search for min_split</summary><ul id="c178dbde-dff1-4c40-9d62-63f409a520b1" class="toggle"><li><details><summary><strong>Config : ./config/classification/wine/test2.ini</strong></summary><figure id="94f8e4dd-cf9b-43ed-94e8-010915452807" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.00.22.png"><img style="width:606px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.00.22.png"/></a></figure><p id="58fc6ae7-8abc-451b-acdf-2f9c4dc4015b" class="">
</p></details></li></ul><ul id="1b1463bf-5ee8-4513-9714-ec6b01d0ae6a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="f5d4535b-e7ce-4d2a-ae01-21c0ce178099" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_16.57.25.png"><img style="width:1176px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_16.57.25.png"/></a></figure></details></li></ul><ul id="6227036b-db59-4f78-9512-1a1b0d1fc322" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="0ad884ef-6886-4367-8006-5d52385a04ec" class="">There is little change in performance with min_split set to anything below 5, although performance is best with min_split &lt; 3. Increasing the min_split degrades performance after this point.</p><p id="901259ff-b89c-415b-9901-5e550156065e" class="">The smaller the value for min_split the more likely we are to be overfitting, and the more likely we are to see 100% for Score . As we saw in test1, with the default settings, we do overfit. The numbers are slightly lower here since we are using cross validation and averaging the scores of the &#x27;left out&#x27; fold, so we see results much closer to the test results of test1 i.e 93%</p></details></li></ul></details></li></ul><ul id="dddc1b7e-2bc9-4f53-8acb-4f45e85db73e" class="toggle"><li><details><summary>Test 3 - grid search for max_depth</summary><ul id="13ae787a-e1d6-431c-b68e-58d2e2d5c548" class="toggle"><li><details><summary><strong>Config : ./config/classification/wine/test3.ini</strong></summary><figure id="bed1d7ec-955d-45da-9195-52349d9911e2" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.05.37.png"><img style="width:588px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.05.37.png"/></a></figure></details></li></ul><ul id="e103dd73-a6b9-47ac-a055-4cea6e1e88d5" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="b2df9b53-1255-4947-8cf7-4e2682053353" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.05.05.png"><img style="width:1200px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.05.05.png"/></a></figure></details></li></ul><ul id="988bfbc3-22b5-438f-aec5-6600224201c7" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="bacf0723-4645-4231-ac5a-f2853230fbb5" class="">For very short depths (i.e tree stumps at depth =1) there is a serious drop in performance, and in fact it&#x27;s not much better than chance at 57%. However quite quickly the depth of the tree is not impacting results after a depth of 3 where we see 92% (same results as before when using min_split of 25, the default as used here).</p></details></li></ul></details></li></ul><ul id="fdff7d3c-553d-4755-bdfc-35d8931ef500" class="toggle"><li><details><summary>Test 4 - grid search for 2 params at once </summary><p id="911326da-8fa0-4dc4-9444-044ad830dd91" class="">These 2 parameters can be &#x27;searched&#x27; for at the same time too </p><ul id="0636ed59-5474-4133-984c-1372707ab9ea" class="toggle"><li><details><summary><strong>Config :</strong> .<strong>/config/classification/wine/test4.ini</strong></summary><figure id="2e344e42-8336-42a0-bc48-166f9c3432ce" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.17.36.png"><img style="width:720px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.17.36.png"/></a></figure></details></li></ul><ul id="97dc9176-5fb1-4a3d-a726-60b17b54fe9e" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="370d0426-8287-4079-8ae7-0075ec726dfb" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.17.07.png"><img style="width:1200px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.17.07.png"/></a></figure></details></li></ul><ul id="d3ff14cf-6506-4dfe-88f0-794bc39e6673" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="53d784d4-849e-49a7-9c28-b6b45b68d326" class="">Looking at the heat-map plot, we see this matches earlier tests i.e that is to say the best results are where min-split &lt;3 and max_depth &gt;3 (with very poor results occurring at max_depth 1  and somewhat poor results with this set to  2) . (Note, the full report included with the source contains full detailed results for the plot above.)</p></details></li></ul></details></li></ul><ul id="0034bc58-0b68-45a4-bc22-9d5d559e2a46" class="toggle"><li><details><summary>Test 5 - baseline with best parameters</summary><ul id="df5cc9bd-c814-4846-ab4c-2240fec890ac" class="toggle"><li><details><summary><strong>Config</strong> : .<strong>/config/classification/wine/test5.ini</strong></summary><figure id="12c522f7-668b-4c03-aefa-f18ec7a97de3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.25.30.png"><img style="width:477px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_17.25.30.png"/></a></figure></details></li></ul><ul id="eda02423-2ef2-4842-a98d-ded0193e632c" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="e892e863-2c3d-4e54-99aa-e2b64068159c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.08.13.png"><img style="width:1200px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.08.13.png"/></a></figure></details></li></ul><ul id="edeee77e-fd9d-4cb7-8fb6-c8837a84fd62" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="0ac2ec05-fcab-464d-b807-12d49c2c6b0e" class="">Despite seeing some small improvements in some of the grid tests (based on cross validation test results) in practice as long as the min_split level is not too high (say below 20) and the max_depth is not loo low (&gt; 3) the best results for this model remain fairly static at 93% accuracy for test data. It still only uses 4 of the available 13 features.</p></details></li></ul></details></li></ul><h2 id="f9025fe8-801f-4b1d-82ff-466043835884" class="">2.2 Ensemble 1 - Bagging</h2><p id="410574b2-3091-4167-859a-611b550cb9ff" class="">Moving on from the base Decision Tree, now looking at extending this to use Bagging instead, with multiple simple decision trees used</p><ul id="ddb93dec-3078-4033-b90d-98f577e586b5" class="toggle"><li><details><summary>Test6 - bagging with (mostly) default parameters</summary><p id="a5770407-aa71-433f-a046-e60cebbb6b15" class="">For bagging we do need to specify at least a setting for the number of trees (number of base models) to add into our ensemble. </p><p id="2460352e-cce6-4738-b9ba-88e1cffb8894" class="">The other Hyper Parameters for bagging are the same as for the base Decision Tree.</p><p id="d43367d4-35e5-4bf4-b08b-ee4333ea5d60" class="">This test will keep all the defaults for the base tree settings, and choose an arbitrary number of tress as 5 to get a baseline to work with.</p><ul id="1ba5f9f9-6b26-4e93-880e-3cfaa7e1e281" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test6.ini</strong></summary><figure id="58d251be-6300-4151-a128-40f6696278c4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.12.37.png"><img style="width:470px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.12.37.png"/></a></figure></details></li></ul><ul id="75ac0c9c-1f86-480f-baf4-bb82e76e2a62" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="fb198e8c-1038-4d32-8a4c-18d8d6d4c5ee" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.11.58.png"><img style="width:1165px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.11.58.png"/></a></figure></details></li></ul><ul id="147077ca-1f49-4475-9046-9a767c7f496a" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="15bf09a6-54a5-4bfe-b782-e5fbdb7c9ca1" class="">With only 5 trees in the ensemble this model does not perform as well as the base DT (93% on test), reaching only 99% on training, 87% for out-of-bag, and 89% for test data. </p><p id="e6cdfa65-ce7b-424d-810a-e4594b0e0fc8" class="">What is interesting is that the Feature Importance results are also very different. Partly this is down to the fact that the bagging algorithm is more likely to choose a larger number of variables to split on based on the fact it is selecting a new bootstrapped dataset each time, and as such some (previously important and dominant) features could be less useful for the subset of data being used. </p><p id="54b2b490-3eab-486a-adeb-2faa90421299" class="">The most dominant features (Proline, Flavonoids, Color Intensity, 6,9,12) are still the dominant features, but Hue and OD280 are also now prominent, replacing the previously fourth placed Alcohol. Perhaps it is the use of these less useful features that is resulting in our poorer performance?</p><p id="5406c1d5-e92e-4f59-ae39-f74c62ee6e9d" class="">Next, it would be good to get a better idea of how many trees we should add to this model to get better results, so we will do a grid test on num_trees for this model, keeping the other settings the same for now.</p></details></li></ul><p id="99f99dbe-2d0f-43a2-8c4e-775d53514da3" class="">
</p></details></li></ul><ul id="244997e5-1d34-4bb1-bbd8-45f7253d50fc" class="toggle"><li><details><summary>Test7 - bagging - grid search to find num_trees</summary><p id="0a1a98d3-ad0e-4d80-900a-f808e3b552de" class="">The list of values tested here starts with our poor first guess of 5 and goes up to 45. </p><ul id="0cb93697-88c2-4638-9b46-b73c16cdd51e" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test7.ini</strong></summary><figure id="3addc62a-4db2-454f-8993-ee7beb7401f3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.26.58.png"><img style="width:431px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.26.58.png"/></a></figure></details></li></ul><ul id="4b71aade-0d1b-4b21-89b6-1d1953429ba2" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="dad42154-5228-4398-be9c-78c4c6d302a8" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.26.37.png"><img style="width:1161px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.26.37.png"/></a></figure></details></li></ul><ul id="b56f6ca5-40d6-4be4-abb0-e346992ad77b" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="847a5939-6cc8-4e4e-8d8e-0ee72244a9e9" class="">Looks like there is benefit in increasing the number of trees at least up to 25. Things seem to drift off after that point.</p><p id="f30c888e-0c99-489a-8744-676342468861" class="">Why would it go down if we keep adding in new models?  Is this overfitting ? Note the results above are on cross-validation averages, so are not showing the pure training results, which may be expected to continue to go up.</p><p id="6b656015-4178-40a3-9bb8-b5a006b6b789" class="">Let&#x27;s run the next test, test 8 , to do a single model report that shows training and test results, with settings for 50 trees to see what is going on.</p></details></li></ul></details></li></ul><ul id="cc7baaef-acef-456c-9d0e-57daf35ce549" class="toggle"><li><details><summary>Test 8 - bagging - is having too many trees resulting in overfitting? Try 50 trees.</summary><p id="1a965a2e-d0b5-4340-9485-b2603c9631fe" class="">Just set num_trees to 50 and use defaults - run a report for training and test results. </p><ul id="e9fc242d-aba1-4bf3-8672-bc2547d18e23" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test8.ini</strong></summary><figure id="6250ab9d-8257-4d3d-8246-63cfd00541cb" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.37.54.png"><img style="width:626px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.37.54.png"/></a></figure></details></li></ul><ul id="145889fa-8471-461b-810c-fb6665b027ff" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="2662453f-38e1-4c87-bd02-1902299df336" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.37.39.png"><img style="width:1152px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.37.39.png"/></a></figure></details></li></ul><ul id="36362ca9-0cff-40c2-9d14-51d2208a4908" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="c279a4ec-0a70-40b9-91c7-7e1ce6511942" class="">Clearly looks like, since training is coming in at 100%, and in the previous grid search we saw results dropping at 50 trees, that we are overfitting here.</p><p id="2140b712-247d-4179-b773-52e73a404f41" class="">However, the other thing of note is that the bagging ensemble is outperforming the base DT, at nearly 95% instead of 93% in the first test.</p><p id="aba36ec0-80a3-43c7-8002-1d50106b8777" class="">Maybe we could improve this more if we restricted to the 25 tress, which was the best result suggested by test 7 (which showed cross validation scores in the 97% range.)</p></details></li></ul><p id="d8cba4fa-6e86-41c2-a5a9-5b1b79b695f6" class="">
</p></details></li></ul><ul id="3021e13d-10f2-4c98-91d7-bd3b7af61292" class="toggle"><li><details><summary>Test 9 - bagging - with just 25 trees to try and get the best test results</summary><ul id="15dce2f3-7f92-45dd-9ecf-6ee14574f83c" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test9.ini</strong></summary><figure id="9c961e9d-3b31-4252-9cfb-027f2ca02b2f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.43.52.png"><img style="width:619px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.43.52.png"/></a></figure></details></li></ul><ul id="6b0f116c-e433-491d-9ece-4a19fa58e8b1" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="c39bbf69-b003-4a1b-aff8-0a4bbce80cfa" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.43.35.png"><img style="width:1344px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_18.43.35.png"/></a></figure></details></li></ul><ul id="13d60c65-a7e9-437f-bf89-1ae42e73f972" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="27db4fe9-18a4-4ec7-87c0-a7ddc239fbb1" class="">The best score we get on test data is 96.61% using 25 models in our ensemble, which is already a big improvement on the baseline DT (93%) </p><p id="4191e87e-f044-4e74-b2f5-0cecfbb23f24" class="">What about the other params? The next test will look at using the best num_trees (25) and look for a suitable settings for min_split and max_depth.</p></details></li></ul><p id="f00b1861-c13e-4716-8431-ec55a5caf51c" class="">
</p></details></li></ul><ul id="18503dbd-48a4-4bee-be23-7fcbd203b4f2" class="toggle"><li><details><summary>Test 10 - bagging - 25 trees, grid search for min_split and max_depth</summary><ul id="73a5a71d-e915-4a5c-a036-fe385a0151c5" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test10.ini</strong></summary><figure id="fa386e4f-11a1-4d72-8763-302f012285b9" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.09.17.png"><img style="width:624px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.09.17.png"/></a></figure></details></li></ul><ul id="2bf77901-c126-4abd-8382-8f52ff085960" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="46f93975-4440-4729-be90-be8a131a3314" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.08.51.png"><img style="width:1156px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.08.51.png"/></a></figure></details></li></ul><ul id="24f4d00e-95c4-4af0-8620-af52c818a94e" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="454ec642-8332-485c-a0e5-aa0a479dea8d" class="">There does appear to be some benefit at putting min_split up to 25, as that yields a better score. This is not in line with what we did for the baseline DT where we put the min_split to &lt; 3 for best results.  However, looking at the data in detail suggests that the real conclusion here is that min_split is not having a big impact on the results overall; this could be down to variance in the data (and stochastic nature of the grid search) with other results below 25 being all very similar. </p><p id="02b30249-5ca6-4384-a5e5-c73c580aa1f3" class="">The other parts of the results here do fit into what we could see with the baseline DT; max_depth needs to be &gt; 3 </p><p id="3e8ab44b-2498-4716-a17a-5b408746aab3" class="">The next test will repeat test 9 but with : max_depth = 4, and min_split = 25 to see if the overall score improves.</p><p id="57fae381-7e84-416c-8adc-46733989d882" class="">
</p></details></li></ul></details></li></ul><ul id="4d22c55f-e087-441a-8694-f565f821f32c" class="toggle"><li><details><summary>Test 11 - bagging - 25 trees, max_depth = 4, min_split = 25</summary><ul id="f2e1c0fb-7ae1-4e40-b099-717c397f23e1" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test5.ini</strong></summary><figure id="d160c918-fd29-4a9c-b5d7-ace617eeb99c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.19.36.png"><img style="width:495px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.19.36.png"/></a></figure></details></li></ul><ul id="f3572375-c5aa-447d-9340-1751abe4747b" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="70a3a6b9-4d49-4db5-93c7-e893ac19b7ca" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.20.01.png"><img style="width:1104px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-08_at_20.20.01.png"/></a></figure></details></li></ul><ul id="658393f7-4b05-48d5-8804-5b07075d421c" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="d2a3bccc-5258-40b0-b8a7-2804cf723632" class="">The results were the same as test 9 i.e 96.61%, seems to be where we top out for our test data, suggesting the slight increase from the grid search is just due to variance of the data when doing our cross-validation. The errors only come down to 2 samples being misclassified.</p><p id="ff788230-08c4-4e2f-9cab-ffe933a5e0ed" class="">
</p><p id="3ce527e2-6d6f-4f90-b352-528ddfe732a9" class="">The other aspect that has changed with bagging compared to the baseline DT is the use of more features; whereas the baseline only used 4 features with Bagging 10 of the 12 features are used and contribute to the final predictions.</p><p id="33d6fe15-ce57-4fc7-a07c-99cf13711415" class="">To recap, the feature importance results were as follows:</p><ul id="41dbc37b-6d98-4098-9464-252749675ba4" class="toggle"><li><details><summary>Baseline feature importance scores: </summary><figure id="1b9d0c68-e5ab-4604-81ac-d15081be515c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-24_at_17.18.53.png"><img style="width:559px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-24_at_17.18.53.png"/></a></figure></details></li></ul><ul id="7bb7dd70-5941-45c6-8bf4-4b586caf7220" class="toggle"><li><details><summary>Bagging feature importance scores:</summary><figure id="05a048bc-0f5e-4a7a-8f78-d310fcb31f20" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-24_at_17.19.15.png"><img style="width:567px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-24_at_17.19.15.png"/></a></figure></details></li></ul><ul id="99c9860c-8bc7-4536-9b1c-0d931af9b75a" class="toggle"><li><details><summary>Feature Names</summary><figure id="908da7b7-db68-495d-997b-c309541bd8be" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-24_at_17.20.01.png"><img style="width:269px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-24_at_17.20.01.png"/></a></figure></details></li></ul><p id="5f2c51d7-1882-416c-9174-c9b2c1dd899e" class="">
</p><p id="33154e44-e633-4e4d-b0af-f15038ba2f02" class="">On the whole there is agreement that feature 6(Flavonoids) is strongest, and feature 9(Colour Intensity) and 12(Proline) are also important, though the bagger would put 12 above 9, and also rates 10 and 11 higher than the remaining ones. </p><p id="fafb4779-e11f-4707-ab20-c3ef122a1def" class="">Question - what would happen if we removed some of the features that seemed less important and re-ran the bagger? The next test will look into this.</p><p id="093b270c-4414-44c0-92ab-7070ea3313f4" class="">
</p></details></li></ul></details></li></ul><ul id="a9c7ce9f-5f86-4dcc-a36f-47d8f4599a51" class="toggle"><li><details><summary>Test 12 - bagging - with reduced features, best settings</summary><p id="c3091028-2a1b-4ba5-8d16-1003cfbbf086" class="">The idea of this test is to try to run the Bagging ensemble with our best parameters so far, but removing some of the features e.g just keep the top 5 features, and remove the rest. </p><ul id="49be0654-0446-4beb-ba6c-247a44e51072" class="toggle"><li><details><summary>Top 5 features are :</summary><ol type="1" id="9ec60549-1f11-45cd-a420-65312af5669e" class="numbered-list" start="1"><li>Flavanoids</li></ol><ol type="1" id="c9cc7d18-9bca-428d-a5c7-8e6cdb4f5e08" class="numbered-list" start="2"><li>Proline</li></ol><ol type="1" id="8f2d2f5f-967c-4ad8-b344-8241252e05e2" class="numbered-list" start="3"><li>Color intensity</li></ol><ol type="1" id="a768d151-80e4-4255-9d5b-ac828e229552" class="numbered-list" start="4"><li>OD280/OD315 of diluted wines</li></ol><ol type="1" id="2e3be42d-5567-448e-b490-4317ca5dc370" class="numbered-list" start="5"><li>Hue</li></ol></details></li></ul><ul id="a82f90d2-bc7f-4214-831b-071341b1c437" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test12.ini</strong></summary><figure id="345231ce-9ac3-45b5-9537-72051ef70a1b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.37.35.png"><img style="width:720px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.37.35.png"/></a></figure></details></li></ul><ul id="9574f303-6804-49c0-8702-58be13a0efe7" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ade94355-1bef-4cce-a50f-d4c0da543877" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.38.46.png"><img style="width:1200px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.38.46.png"/></a></figure><p id="56c276c7-275e-4ec9-9a92-6539d5eeaaf2" class="">
</p></details></li></ul><ul id="7f31f061-8c87-471b-983a-fee635a147b6" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="e0c4fb5d-9160-4a2a-83ef-40f87f3fb4f0" class="">Removing some dead wood features was a nice idea but didn&#x27;t improve things - we can try this again later for other models.We did manage to get as good results by only using a subset of all features. With a much bigger dataset this could save a lot of computation performance. </p><p id="a0877d94-efa2-4fb9-98a3-ceea49ac7d11" class="">In the baseline Decision Tree only 4 features were used. Could also try to only keep in the best 4 features? i.e also exclude : Hue which is currently the 5th best deature, and was not used at all in the base DT model. </p></details></li></ul></details></li></ul><ul id="4dbc6edf-0fcb-459b-8de0-373da11b8b4e" class="toggle"><li><details><summary>Test 13 - bagging - top 4 features only, best settings</summary><ul id="20a71820-9e26-4cf0-a735-f31a5b91d04e" class="toggle"><li><details><summary><strong>Config</strong>:.<strong>/config/classification/wine/test13.ini</strong></summary><figure id="ec766437-19e8-4ad4-9f47-8e9602bf7add" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.44.33.png"><img style="width:864px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.44.33.png"/></a></figure></details></li></ul><ul id="e546d6af-eecd-4e93-80eb-10737aca730e" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="530c504a-395c-4c9a-a571-74763996867f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.42.21.png"><img style="width:1200px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_13.42.21.png"/></a></figure></details></li></ul><ul id="70df6000-5e77-4bff-908a-313e9a1507a5" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="26b2a5c2-86bd-4692-9e46-c491b9041d7c" class="">This is interesting as it does confirm that Hue is an important and useful feature, which was not even used in the base DT model. One of the advantages of Bagging is that the random nature of the dataset being bootstrapped results in the ensemble model &#x27;exploring&#x27; features that would otherwise be missed using the simpler &#x27;best&#x27; split employed by the decision tree. </p></details></li></ul><p id="f57c2885-b4dd-42cd-8173-629a2d763efd" class="">
</p></details></li></ul><h2 id="c81ffaa4-4978-4233-b9a3-328547cd0415" class="">2.3 Ensemble 2 - Random Forest</h2><p id="2c95be69-b220-4c69-a300-f5ada42b9b78" class="">Repeating tests done for the bagging ensemble, but using the Random Forest approach instead. </p><p id="ac71c644-640e-4118-b0b6-f4d112593be8" class="">For Random Forest the same base hyper parameters are applicable, but has the extra on : <strong>num_features</strong>. </p><p id="5c6c9a05-96c9-4b9c-bd1f-e689e54cb77f" class="">This allows us to tune the model choosing how many (randomly chosen) features should be included for each base model. This is a mandatory new hyper parameter for the RF model</p><ul id="80c3fd19-848f-4c2a-b435-adfd62444fe2" class="toggle"><li><details><summary>Test14 - Random Forest with (mostly) default parameters</summary><p id="b5438f95-9bc7-49cd-89ce-486d40687629" class="">As with Bagging test 5 this test will keep all the defaults for the base tree settings, and similarly choose an arbitrary number of tress (a mandatory parameter) as 5 .</p><p id="939d899d-13a3-4cef-8b32-4fed02c2ca91" class="">For the new parameter of num_features, since this data set has 13 features in total we will choose roughly half and set it to 7</p><ul id="6a021a4e-52e3-4445-b737-09ab6d192c29" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test14.ini</strong></summary><figure id="a5a11842-ca20-485b-be02-1bb91818ae2e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.19.36.png"><img style="width:461px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.19.36.png"/></a></figure></details></li></ul><ul id="5a31a5c1-cc5a-44ba-8f90-88198515b327" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="74f9b81b-5973-48ad-883c-4bec24796665" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.19.08.png"><img style="width:1248px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.19.08.png"/></a></figure><p id="ff52a3fe-8d0c-49b1-8088-4c53ee4d4240" class="">
</p></details></li></ul><ul id="3e559193-498f-452a-9b3b-7370c2b4f367" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f6542613-b70f-4117-a592-5f6699a9caed" class="">At 94.92% test prediction accuracy this ensemble performs better than the baseline DT(test1) and the baseline Bagger (test5), and better than the best DT, but with current parameters not as good as the best Bagger (test 11, 96.6%) </p><p id="1814648d-c13f-49b7-b18f-ecf05d977017" class="">What is also interesting is that the feature importance results are also very different. Alcohol, which was used in the baseline DT but not rated highly with the Bagger, is now rated as 3rd most important feature. Also Flavonoids which was rated highly for both the other models is now pushed in to 6th place. Overall more of the features are being used here (11 out of 13 making a contribution compared to 4 in the DT and 10 in the Bagging case). </p></details></li></ul></details></li></ul><ul id="0dd8b7fb-3320-4f1f-ada4-aba796dfef95" class="toggle"><li><details><summary>Test15 - Random Forest - grid search to find num_trees</summary><p id="da7b9e38-5bd6-4c6b-8b81-f05a4da5762e" class="">The list of values tested here starts with our poor first guess of 5 and goes up from there to see if more trees will improve the performance. </p><ul id="9cbe192f-73e6-48af-beca-50ee53b51078" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test15.ini</strong></summary><figure id="f025f8bd-ba8c-4375-9c04-13b44f7a56d2" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.32.34.png"><img style="width:505px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.32.34.png"/></a></figure></details></li></ul><ul id="e25868fe-7e2e-46ef-9ce8-22652e63ceea" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="c44f2b10-789a-403e-bb9d-f10cb5716ce5" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.32.12.png"><img style="width:1152px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.32.12.png"/></a></figure></details></li></ul><ul id="f3b2cfda-c52c-496b-b54c-192ef6ff8133" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="b49aca05-188e-4485-ae94-530c92fd93da" class="">Like the Bagging model, the RF model sees improvements with more base models added, but these results seem to suggest that we want a much higher number of trees to see the best results. The best results occur with 100 or more trees (getting impressive results on the averaged cross-validation scores of 98.26%). We&#x27;ll pick 100 trees as our parameter for this model and this dataset. </p></details></li></ul></details></li></ul><ul id="34b2c98b-17f4-40a2-9dbf-af3864bdd8a2" class="toggle"><li><details><summary>Test16 - Random Forest - grid search to find num_features</summary><p id="2e48c352-2422-48ee-b647-0985ad371f75" class="">Keeping the new value for num_trees at 100, this test will investigate the best value for num_features which we arbitrarily set to 7. We have up to 13 features to choose from, although choosing 13 would in effect be the same as Bagging. This test will try all combinations 1-13.</p><ul id="780d57fa-d26c-42b7-9725-6f9c63f3adf9" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test16.ini</strong></summary><figure id="e15410b3-a998-48fa-accd-18f81c414976" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.37.17.png"><img style="width:515px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.37.17.png"/></a></figure></details></li></ul><ul id="e7ea47bb-790a-44ba-ae28-8083164db3f6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ae844d87-981b-4601-8c55-cd10397e8d46" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.43.53.png"><img style="width:1296px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.43.53.png"/></a></figure></details></li></ul><ul id="ee216100-6be3-41e2-a97f-5c84ec68e2bb" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f376dd15-9aab-414d-9bcc-b766f47365ed" class="">Results are a little bit inconclusive except to say there is benefit in having &gt;3 features and less than 11 features. Within this range the results are quite similar (with an oddity around 8 features). We will pick 6 features as our setting here</p></details></li></ul></details></li></ul><ul id="81691701-5959-4a81-8f6a-2fbaf59b9a4c" class="toggle"><li><details><summary>Test17 - Random Forest - grid search to find max_depth and min_split</summary><p id="dc9e388d-6939-4ef0-be17-c273acdb18e4" class="">Up to this point we used defaults for max_depth (which does have a high default at 1000!) and min_split is 25 (which has actually been ok for other models). In this test we will grid search both of these at once to see where the best combinations are.</p><p id="088d0630-818c-4915-8775-dec6f15fd9af" class="">
</p><ul id="310f0135-b65e-47f4-867b-97f5754a5d0d" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test17.ini</strong></summary><figure id="c3ac0aa8-11a6-4e76-a8e3-caf0ddeef6f9" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.49.19.png"><img style="width:480px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_15.49.19.png"/></a></figure></details></li></ul><ul id="e2a58b49-0c25-40d8-ba7a-d190311cdd23" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="0d43c94b-070b-46c9-a7a6-1af6392d5cc4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_16.27.45.png"><img style="width:1156px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_16.27.45.png"/></a></figure></details></li></ul><ul id="0c8136ec-0118-46bf-b35a-30b9e044fddf" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="e80193a5-c820-40d9-a189-8bd410d4fae5" class="">The results suggest that max_depth should be &gt; 2 (3 looks very good, not sure why), and min_split should be &gt; 4. We&#x27;ll choose: min_split: 3. max_depth: 5</p></details></li></ul></details></li></ul><ul id="fbb6c546-3ee3-4631-a408-734433c970a9" class="toggle"><li><details><summary>Test18 - Random Forest - with best parameters found so far</summary><p id="182c765f-6a78-4883-bc50-c542a7a710f0" class="">We will use the best paramters from the test above and see what change we get in overall test accuracy. </p><p id="ba443e9c-f15d-42e5-958b-854438655943" class="">
</p><ul id="ee3f81a5-c7f1-40f2-97cc-8be7fc04d1a0" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test18.ini</strong></summary><figure id="11f2fe25-ebc7-44ba-a0f9-2d5d940efe6b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_16.36.25.png"><img style="width:357px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_16.36.25.png"/></a></figure></details></li></ul><ul id="226ce098-91b9-4e98-bab7-8c403e8b64ac" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="433a1889-f48b-49b6-971d-ef093a9ac08c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_16.36.08.png"><img style="width:1177px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_16.36.08.png"/></a></figure></details></li></ul><ul id="78a50172-dd63-4a76-bf2d-a0bc9f807885" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="54ca94ce-7fef-4bb6-bbdf-b7d53f11a7b1" class="">The test results came in at a very impressive 98.31%, the best we have seen for any model so far. </p><p id="e63029e3-9904-45da-b897-7d1136166c16" class="">Now that the model is tuned, we will review the features being used by this model: </p><figure id="ccd381b6-ecf4-4547-94aa-10c063d1bd9b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-25_at_12.56.02.png"><img style="width:1008px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-07-25_at_12.56.02.png"/></a></figure><p id="051cda5b-abfd-4e04-98bd-76445df49014" class="">The top 5 ranked features are similar to the Bagging results (10 and 11 swapped places but seem to have a more confident place in the top 5 now.) Alcohol which was used in the base DT is also shown to be a strong player outside this top 5. Interstingly all the features are now playing a part and contributing to the overall prediction; this is the first time we have seen this.</p><p id="8edb766c-5c20-4ea4-b5a0-bc52cba0fef0" class="">If we repeated the previous test for bagging, to reduce features used to top 5, since our RF setting num_features is 6 we would end up using all 5 features, and end up with the same results as. Bagging (94%). </p><p id="6b66d64d-0ace-4b60-a2ba-ad668cb56284" class="">Although perhaps less important these other feature are allowing the model to get a much better score than Bagging; you can see all features are contributing something to the results.</p></details></li></ul><p id="ac3c766f-d36d-4372-a2bb-4e461b2bf7c0" class=""> </p><p id="fd2f2589-71ed-4c27-ae35-f2987172693e" class="">Although perhaps less important these other feature are allowing the model to get a much better score than Bagging; you can see all features are contributing something to the results</p><p id="bad83e03-0344-45f2-97d3-7e7aee551c59" class="">
</p></details></li></ul><p id="2bc35340-80a9-4537-8f32-31a339e46058" class="">
</p><h2 id="cfc0d4df-7b6c-40d8-a002-42b9801d5eaa" class="">2.4 Ensemble 3 - Boosting (AdaBoost)</h2><p id="b610a7cf-6214-48bf-8015-a98c2484b038" class="">Repeating tests done for Bagging and Random Forest, but this time using AdaBoost. </p><ul id="ea89ffc8-73fd-477a-b1f8-7f7b578ca4cc" class="toggle"><li><details><summary>Test19 - AdaBoost -baseline parameters</summary><p id="f9b16d23-1c7b-46e4-a7b6-02f174cf21d9" class="">The hyperparameters for AdaBoost include all the parameters for the base Decision Tree, and also include a mandatory one for num_trees (similar to bagging and RF)</p><p id="b3be1416-db3f-4407-967d-445ec3f63445" class="">The default value for max_depth for AdaBoost is 1;  the default behaviour is to use weak learners by using Decision Tree Stumps (one split only). This differs from Bagging and RF where the code defaults have been set to 1000 (i.e deep trees)</p><p id="ccb74a9f-49f3-4f70-a6c5-941c29890067" class="">This test will use an arbitrary number of tress (5) to get a baseline AdaBoost model to work with.</p><p id="fea81f79-389b-4490-bdd8-b9887c0adffc" class="">
</p><ul id="a10cbd30-23ff-4f03-ab9b-2f79f3a6ccf3" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test19.ini</strong></summary><figure id="a94d8b6e-6945-4247-b461-879581e5fda4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.31.00.png"><img style="width:360px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.31.00.png"/></a></figure></details></li></ul><ul id="e9e53b9a-2d83-41c1-8c22-143a711ac79a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="85b78933-82d9-4eb5-aa12-a581a39141ea" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.30.02.png"><img style="width:864px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.30.02.png"/></a></figure></details></li></ul><ul id="710a32d2-6f2a-4142-8fc1-6dc9d0abe6ee" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="d49dd080-6114-4932-a9fb-163225acfa57" class="">This ensemble does not perform as well as the base DT with only 5 trees in the ensemble, reaching only 97.48% on training and 88.14% for test data. </p><p id="3efd8576-5e19-41c0-abe4-6cd6434fc11e" class="">What is also interesting is that the feature importances are also very different. Some are familiar by now (6,10,12) but 9 (Colour Intensity) is not usually as highly rated . Only 4 features are being used.</p><p id="d978fe32-d658-4257-9a92-8416637a7782" class="">These initial baseline settings do not look good. </p></details></li></ul><p id="f55956da-50b6-4754-8371-5cde0ac0be31" class="">
</p></details></li></ul><ul id="460f843c-b26b-4019-83d8-e568c1a2107f" class="toggle"><li><details><summary>Test20 - AdaBoost - grid search to find num_trees</summary><p id="8278e358-860f-4437-82ee-83693f42f81f" class="">The list of values tested here starts with our poor first guess of 5 and goes up to from there.</p><ul id="efed5460-56e5-40bd-97da-a416d8f134c5" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test20.ini</strong></summary><figure id="c1ee1b90-f89f-45c3-9b4d-cb43925d0f01" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.39.42.png"><img style="width:454px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.39.42.png"/></a></figure></details></li></ul><ul id="5c41a6a1-6446-447d-8cdf-6176a10e6767" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ff020860-e233-40c6-b4c9-35a289a0751e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.41.54.png"><img style="width:1176px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_18.41.54.png"/></a></figure></details></li></ul><ul id="0b976028-7929-4085-b6ff-d954a745c691" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="18b6b84d-51f7-45dc-abf9-605694090a40" class="">Adding more trees helps improve results with CV score up to 95%. Trees above 20 seems to be good. the peak at 20 seems a bit odd, but that could just be a quirk of the stochastic nature of the cross validation process. My conclusion for the graph above is that having num_trees somewhere between 50 and 200 should get good results. Will we set it to 100. The next test will repeat test 19 with this new paramter.</p></details></li></ul><p id="18c6e95c-78ed-4d9e-9674-4f57aba0d8aa" class="">
</p></details></li></ul><ul id="88dd472e-01be-4f64-a29a-bf7fcd3b3a14" class="toggle"><li><details><summary>Test21 - AdaBoost - baseline params , plus num_trees set to 100</summary><ul id="52d312e0-a396-4477-8535-70b91e372899" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test21.ini</strong></summary><figure id="51a3083e-32ae-49cf-8f3d-647db2c3c75a" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.40.49.png"><img style="width:538px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.40.49.png"/></a></figure></details></li></ul><ul id="7408c2b5-f8e3-4846-aa6e-7e181cdd6027" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="bf3cda87-06f9-4767-bf8b-b9a664a88066" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.40.26.png"><img style="width:1175px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.40.26.png"/></a></figure></details></li></ul><ul id="22042931-80a0-4160-bee1-859de182a7bb" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="3f9a970a-6ec3-4bfc-aa1d-8fe0f4b2ed56" class="">These results are much better with test accuracy now 93.22%. This is now better than the baseline DT, but not as good as the best Bagging or RF model. Next we&#x27;ll look to tune the max_depth; can we get better performance if we use trees larger than just stumps?</p></details></li></ul><p id="25e41fca-d318-48fb-9970-5e4594fcd85b" class="">
</p></details></li></ul><ul id="34d97d5e-be6c-42eb-9f4d-48bd70a6b5e1" class="toggle"><li><details><summary>Test22 - AdaBoost - grid search for best tree depth </summary><ul id="782caaba-7634-4db6-9026-509999b66d74" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test22.ini</strong></summary><figure id="7fc397ba-6131-4e41-a671-9f6947694210" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.46.01.png"><img style="width:447px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.46.01.png"/></a></figure></details></li></ul><ul id="7817c0c4-1ba5-43c8-8b3f-2a11557ecace" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="1b5d0cfb-2d43-471c-a347-52d6f080cf6a" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.48.42.png"><img style="width:1166px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.48.42.png"/></a></figure></details></li></ul><ul id="9ced4e63-4334-4dbd-80e6-e2fb60901a55" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="85dd3536-20bb-43bf-bfe6-08babbcfd187" class="">The results improve with a small increase in the max_depth from 1 (stumps) gaining up to 98% (for cross validation score) . Most of the results are quite similar, with a small spike at 3, which is probably down to stochastic nature of CV averaging. </p><p id="b675375f-2823-470e-9652-647a652dc822" class="">We will choose to go with a modest increase to depth 2 to keep our trees small but get some benefits of this extra depth. </p><p id="436f7ba9-a948-4b77-b86e-81abf2563b5b" class="">Next we&#x27;ll repeat the basic report with these new settings to see what we get on the test set .</p></details></li></ul><p id="63549cce-af69-4847-b741-ac16dd547e80" class="">
</p></details></li></ul><ul id="c5083955-ac57-41f7-87bf-0b8de9d0c842" class="toggle"><li><details><summary>Test23 - AdaBoost - tuned num_trees and max_depth </summary><ul id="7f26ceb7-2340-44a3-a9be-45ca413d231d" class="toggle"><li><details><summary><strong>Config:./config/classification/wine/test23.ini</strong></summary><figure id="4c83655f-e234-4d82-9f38-071e33168727" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.52.15.png"><img style="width:482px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.52.15.png"/></a></figure></details></li></ul><ul id="ec35e55d-cf20-45d2-9bfa-b39836f591e8" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="a2c7ea06-24a0-434d-81a9-202fcbb142fc" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.51.33.png"><img style="width:1171px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.51.33.png"/></a></figure></details></li></ul><ul id="d3d91900-dc39-498c-9f9c-0138423cd777" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="731f2c5b-6e26-41e5-8c81-f3abc601f6c4" class="">The results improve from 93 to 96.61% with depth change from 1 to 2. This puts it performance similar to bagging, and just below that of RF. </p><p id="b92e962d-b895-4cb8-805e-68a8105a91f9" class="">Turning attention to the feature importance scores in this final test, the usual suspects are at the top of the rankings, although a slight re-ordering compared to RF. The noticeable difference is that only 12 or the 13 features have been used here (compared to 13 for RF, and only 10 in Bagging.)</p><figure id="19f98d0b-42a4-4458-b476-4e8a8e5c53a1" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.55.00.png"><img style="width:1440px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-09_at_19.55.00.png"/></a></figure></details></li></ul></details></li></ul><h1 id="281eb5a1-f216-47cd-b109-bcf9e5497e4d" class="">3. Classification - Cars Dataset</h1><h2 id="ca38d7f1-49ed-4c0e-ab96-c27561bf7804" class="">3.1 BaseLine Decision Tree</h2><ul id="c27b571d-7d41-4e1e-9f1a-9b9df8176336" class="toggle"><li><details><summary>Test 1 - baseline with defaults</summary><ul id="73e8f8d3-f413-492c-90fc-863489b7decf" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test1.ini</strong></summary><figure id="094b61e1-552e-4517-9d47-48b645bd674d" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.39.40.png"><img style="width:522px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.39.40.png"/></a></figure></details></li></ul><ul id="c9fa6b41-a120-4f90-82a4-3dcf4dbcc54f" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="928c6b25-5761-472a-954b-d391f8a09eb2" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.33.33.png"><img style="width:960px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.33.33.png"/></a></figure><p id="0c1b0e70-939c-42a3-87ef-4244493d92b4" class="">
</p></details></li></ul><ul id="08c4e8a6-0e6c-4217-89c4-3a9f9e4cc63f" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="907de656-b84c-4bdb-a75a-3329c7adbb99" class="">In contrast to Test 1 for the Wine dataset this result does not show overfitting, although does also have a slightly less impressive test accuracy of 92.64%.</p><p id="aaa924e4-bf44-4ddd-a7d2-26730fa798ba" class="">The other things of note are that the tree is much deeper this time, and that all the 6 available features were used to make the tree (In contrast to the Wine dataset which used 4 out of 13).</p><p id="d94033df-f53d-470a-bc70-66e11b1fdc4a" class="">The next tests will look to find the best hyperparamters for min_split and max_depth to see if the accuracy can be improved.</p><p id="65eb08e2-6c12-4035-8ba3-5920d2659405" class="">
</p></details></li></ul></details></li></ul><ul id="884dcc7f-59ff-4ee7-9b12-58b3ecf17c5e" class="toggle"><li><details><summary>Test 2 - grid search for min_split</summary><ul id="d608f9e9-f89e-4a15-a7da-835539b47ba7" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test1.ini</strong></summary><figure id="4f25367c-2a76-429b-946a-518e7e58ac5c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.45.01.png"><img style="width:600px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.45.01.png"/></a></figure></details></li></ul><ul id="86ce590f-9341-485f-b09d-734ad1853265" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ebe1c88f-0ad8-43ae-956e-75cead2250fb" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.46.23.png"><img style="width:1056px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.46.23.png"/></a></figure></details></li></ul><ul id="e37ce818-4e6b-4447-abb7-d831568bfef8" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="3a369b04-43bb-4736-bf46-090298a608e5" class="">The results clearly show that a low min_split performs better, with &lt; 5 being the best, perhaps peaking at 2. We&#x27;ll set this to 2. </p><p id="3a4d1d7d-0edb-4093-9d0d-11ceca689de9" class="">
</p></details></li></ul><p id="e7854bed-bbc8-475e-a2a0-d1a49650569e" class="">
</p></details></li></ul><ul id="8ecac795-d53a-4dca-bcde-ba1f8f726ca0" class="toggle"><li><details><summary>Test 3 - grid search for max_depth</summary><ul id="514ccf3b-d953-4ec8-be2d-49019e78fa7e" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test1.ini</strong></summary><figure id="0898f3cf-31e7-4f1e-8485-92f2c5ed9aa4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.51.20.png"><img style="width:692px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.51.20.png"/></a></figure></details></li></ul><ul id="0505051f-f244-43cb-8cb1-d3d628c3286c" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="7f35071f-90a0-4686-a379-45e77fadcb62" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.51.52.png"><img style="width:1104px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.51.52.png"/></a></figure></details></li></ul><ul id="f1c3e7a7-15d8-4df7-8578-ec008a3ced7b" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="62bcfd29-5e60-49ff-9b6a-02e472203c09" class="">Anything above 7 seems to get us the best here. We&#x27;ll set it at 8. </p><p id="41f96f4e-85e5-4360-acd7-077e4169fd78" class="">
</p></details></li></ul></details></li></ul><ul id="3c6f6558-791f-42ae-9dfd-3f1f2973715c" class="toggle"><li><details><summary>Test 4 - baseline using best parameters</summary><ul id="2ab6ffbf-5d49-4995-b8de-5403cee78603" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test1.ini</strong></summary><figure id="c3f98115-e1ac-4419-913b-da7f53c30c6e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.53.57.png"><img style="width:485px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.53.57.png"/></a></figure></details></li></ul><ul id="49435ffb-caa2-438a-bdf8-559cc45422ef" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="bf1ad471-b67e-4168-a395-315a5a850b58" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.54.27.png"><img style="width:1104px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_15.54.27.png"/></a></figure></details></li></ul><ul id="124b2899-e0ea-4ff4-8681-2bf08d267ee1" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="3d0ac94f-2244-4764-8e1c-9581a536c483" class="">With tuned parameters the test accuracy has gone from 92% to 97%, which is quite impressive. Again all features are being used as well. Lets see how the ensemble models can compare .... </p></details></li></ul></details></li></ul><h2 id="e36f9c52-a765-49dc-b07f-b056e005bee7" class="">3.2 Ensemble 1 - Bagging</h2><ul id="599768b0-7c73-4549-97cc-b191d6768841" class="toggle"><li><details><summary>Test 5 - bagging with (mostly) default parameters</summary><ul id="04d25dc2-2c29-4a0c-8140-17c13a7a6cec" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test5.ini</strong></summary><figure id="0ec106d8-b758-488a-a93d-cbba46efc3b3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.01.44.png"><img style="width:467px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.01.44.png"/></a></figure></details></li></ul><ul id="59c92f75-d1fc-4118-a357-8e811f7cac22" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="aa165ce8-6dd1-4abc-b8de-04e40d72db81" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.02.29.png"><img style="width:1056px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.02.29.png"/></a></figure></details></li></ul><ul id="d0341063-af50-43b3-9d4d-155137e99210" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="81398bb1-5411-46f4-b4a5-92ba21d87446" class="">The test accuracy is 92% which is similar to the untuned DT baseline, but a lot less that the best score from the tuned DT baseline of 97%. Tweaking the parameters may improve on that score. The first typical thing to explore with ensembles is the num_trees .... </p><p id="e8b7e963-9943-406c-9593-6d55f817841b" class="">
</p></details></li></ul></details></li></ul><ul id="d37577d5-1a45-4827-b74a-b0cd1c6b24d8" class="toggle"><li><details><summary>Test 6 - bagging - grid search num_trees</summary><ul id="e0066076-555f-4d67-a825-439e2bdd52e1" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test6.ini</strong></summary><figure id="c48cd5f7-0c76-4b2b-9667-dd14fd2ebc80" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.27.16.png"><img style="width:637px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.27.16.png"/></a></figure></details></li></ul><ul id="da45316c-201e-47bd-989f-b12b958adb25" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="22644723-209b-4e26-8213-18a356b2631b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.27.32.png"><img style="width:1056px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.27.32.png"/></a></figure></details></li></ul><ul id="e52bcf43-6a4c-46cd-a419-bea55f0f6657" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="726abca0-9db1-4ff3-8c1f-f57359edddc7" class="">Clearly increasing the number of trees improves the results, and we seem to get good results all the way up to about 500, although there is a plataue between 300 and 500. We will go for 400 here. The fact this dataset has many more samples means we end up using a lot more trees.</p><p id="80909a00-99ac-4d15-ba25-f0e41151e614" class="">
</p></details></li></ul></details></li></ul><ul id="de0deba3-eae2-41f4-970b-d933370366c2" class="toggle"><li><details><summary>Test 7 - bagging - grid search for max_depth</summary><p id="bbce553d-dae5-42ff-a0c2-9d5538e2d9fe" class="">For this test we are using our best num_trees (400),and searching for the best value for max_depth</p><ul id="89c8c96c-ce3c-48f6-87f2-8165382eb40f" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test8.ini</strong></summary><figure id="fc9254b7-a817-4fcc-9339-179af4345c78" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_19.48.19.png"><img style="width:425px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_19.48.19.png"/></a></figure></details></li></ul><ul id="452c24d6-1b97-4b94-b813-fef92d60d9c4" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="423c7066-3cb8-4aaa-b351-4ecc48b98f7b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_19.49.56.png"><img style="width:1167px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_19.49.56.png"/></a></figure></details></li></ul><ul id="4ed529b3-6c02-4971-a53e-da48f995055b" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="8d4ab902-0358-433e-b126-f84f72dff17a" class="">increasing depth improves performance up to about 10 after which the performance is static. We&#x27;ll choose 10 as the value for max_depth</p><p id="5c13d6ce-cd0d-48b2-a169-24b830071f7e" class="">
</p></details></li></ul></details></li></ul><ul id="8c41e296-8302-4ee7-8938-7a50d00d4303" class="toggle"><li><details><summary>Test 8 - bagging - grid search min_split</summary><p id="f965dd27-609a-4833-804d-257d62425c3a" class="">Use the same as test 5, but grid search on min_split</p><ul id="37f41a19-1c7b-43c6-a00b-19b4f54a6850" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test9.ini</strong></summary><figure id="016a31d2-0c43-49e6-900d-3398b40a3552" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.47.47.png"><img style="width:491px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.47.47.png"/></a></figure></details></li></ul><ul id="f55295f8-090a-4d12-bda1-4014f317b48f" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="cfddae05-eb39-4b42-9608-7b5bbc9ffedd" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.48.30.png"><img style="width:960px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.48.30.png"/></a></figure></details></li></ul><ul id="023044d9-9c78-42c8-bd00-811e8d97e359" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="fec2951e-fe01-4d43-a4a7-61e16dfc5fb6" class="">Similar to the baseline DT, min_split of 1 and 2 do best. We&#x27;ll pick 1</p><p id="f03c3ca4-3848-4d23-8031-48e4e73f7f2e" class="">
</p></details></li></ul></details></li></ul><ul id="afddaf9b-15c5-4c0e-b77f-1b2e2aea29f6" class="toggle"><li><details><summary>Test 9 - bagging - using best params</summary><ul id="77e40e81-6a6b-439b-a6f6-f5b53eeca7dd" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test9.ini</strong></summary><figure id="d5d56ee0-ee52-4437-b3bb-d7cd0beb4f2e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.55.58.png"><img style="width:561px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-10_at_16.55.58.png"/></a></figure></details></li></ul><ul id="700c1451-ad57-4203-ba71-048dd00d9f18" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="4376d20f-1b6b-4bcf-83c8-386bdf2cc6de" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-11_at_22.21.38.png"><img style="width:1153px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-11_at_22.21.38.png"/></a></figure></details></li></ul><ul id="475bd9bc-d230-473b-8f45-4c8b612d9029" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="8da1807f-ef2d-43fa-b142-05fb4d811432" class="">With tuning the performance of the bagger has improved from 92% to a much better 96.5%, although it has not improved as much as the baseline DT which managed 97% accuracy.</p><p id="9b0a6224-4902-4373-b45f-05eaa93ff71d" class="">
</p></details></li></ul></details></li></ul><h2 id="fa48c8a1-d45c-4bce-a3a2-53dcc267611c" class="">3.3 Ensemble 2 - Random Forest</h2><ul id="066d4c7a-0f8d-42fa-a5a0-7a6df064441c" class="toggle"><li><details><summary>Test 10 - Random Forest with (mostly) default parameters</summary><p id="10539627-8c0d-49da-95cc-494e28b262ce" class="">Random Forest needs to have num_trees and num_features set as a minimum. For our baseline we choose 5 for num_trees, and since we have 6 features in total we&#x27;ll choose 3 for num_features.</p><ul id="d3f550df-6a30-46df-8889-7cd6d8b267eb" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test10.ini</strong></summary><figure id="eaa7e424-8951-42ae-845d-7dcea982cf18" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-11_at_22.29.10.png"><img style="width:483px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-11_at_22.29.10.png"/></a></figure></details></li></ul><ul id="72f9da32-f0a4-4eba-82fb-46884e713bf6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="3c8f0eb9-60cb-4222-9a8e-f0bbc1bacee5" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-11_at_22.30.51.png"><img style="width:1150px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-11_at_22.30.51.png"/></a></figure></details></li></ul><ul id="b7a5b845-7139-4148-a3c1-e1d55e65abbe" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="eacfd74b-b085-4ead-afbf-2bdb583c32ad" class="">Default parameters coming up with 78% accuracy, and only using 5 of the 6 features. Lets first search for a better number of features trees to use .... </p></details></li></ul></details></li></ul><ul id="037b24b4-b13d-431f-b4c8-aa6a38d5f4bc" class="toggle"><li><details><summary>Test 11 - Random Forest - grid search to find num_features</summary><ul id="6871af50-0935-4c73-865a-0f639bd7b21f" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test11.ini</strong></summary><figure id="2cd4dbb4-7e25-4d63-9932-98cd0958dc3b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.12.00.png"><img style="width:503px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.12.00.png"/></a></figure></details></li></ul><ul id="688f787c-ee11-4df2-99e1-2038586abda1" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="3951884f-6094-456a-84ed-b4e4c1dc5521" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.12.19.png"><img style="width:1019px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.12.19.png"/></a></figure></details></li></ul><ul id="28ed1f1d-bbca-419b-9782-e52a764aa763" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="c3e30d83-d8bd-476a-aec5-7ff9c0e52927" class="">These results show that the best number of features is 6, from a total of 6, which is the same as bagging! This suggest RF is going to do no better than bagging. </p><p id="af6ddbad-46b6-4e4d-8507-9a3c3874f9e5" class="">To confirm this suspicion, I will repeat the test using the params that gave us the best bagging results, but repeat them over a grid search for number of features to see if RF will get any better results.</p><p id="f123fa3e-226e-4ce8-a534-c5c6f8faa524" class="">
</p></details></li></ul></details></li></ul><ul id="b938fad7-c64c-4798-babb-36916de44b03" class="toggle"><li><details><summary>Test 12 - Random Forest - use bagging best params, but grid search to find num_features</summary><ul id="69582bf1-720d-4fa5-88be-5ce8abb8f657" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test12.ini</strong></summary><figure id="4ff60c62-0bd4-43e4-9c95-f635a3949ce3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.24.35.png"><img style="width:672px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.24.35.png"/></a></figure></details></li></ul><ul id="0a7cf8ba-6de6-4434-95f3-d7a586306d38" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="0b412c57-863b-4771-9115-b3f2cd138e3b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.25.03.png"><img style="width:864px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_14.25.03.png"/></a></figure></details></li></ul><ul id="fbfc4c18-e39e-4403-acea-58ec1d690737" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="702927b0-42f5-4749-881c-8f0584856291" class="">This confirms that the best results we get with RF is, in effect, bagging. I.e we will get the same 96.5% since if using all features this is the same as bagging.</p><p id="dbaa72d9-551c-4a1f-822b-e60b2f7d3e5b" class="">This concludes the RF investigation - seems like due to the nature of the data (i.e it is not very well balanced, with one class having a lot of values and the others not very many) then RF does not perform very well (or only as well as a Bagging model.)</p></details></li></ul></details></li></ul><h2 id="092553e0-6eca-4e81-8d96-3fe35195e4eb" class="">3.4 Ensemble 3 - Boosting (AdaBoost)</h2><ul id="699972e5-3d83-45f8-a820-804b3e4aa788" class="toggle"><li><details><summary>Test 13 - AdaBoost - baseline using default parameters</summary><ul id="3a980a45-ae02-463e-817d-f616dab653b7" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test13.ini</strong></summary><figure id="e3349fdd-ce58-4338-bbdb-e000ac1c9f74" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_15.00.20.png"><img style="width:450px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_15.00.20.png"/></a></figure></details></li></ul><ul id="233578f3-a28f-4e85-8a2e-a3aec0799843" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="d60e4a9b-392e-418e-8905-5e9f28ab7d44" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_15.06.00.png"><img style="width:1175px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_15.06.00.png"/></a></figure></details></li></ul><ul id="7bbf1c72-48c6-4b91-959c-fbf17db535f2" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="46745fb5-5181-42f9-afbe-cea384fbeb8f" class="">The results don&#x27;t look good, but not too dissimilar to RF with our defaults. It is only using 2 of the 6 features here.</p><p id="73b22d30-48aa-4f06-9812-1ca138733f15" class="">Next we&#x27;ll try and find a better value for num_trees.</p><p id="27630ff5-35b3-4d2a-8f3d-1ffae646015e" class="">
</p><p id="d9102c2e-6e9f-4b5b-82be-5ef5ff0d6f01" class="">
</p></details></li></ul></details></li></ul><ul id="cf2d31bd-44ab-4a0d-8f35-59d35a4fbb93" class="toggle"><li><details><summary>Test 14 - AdaBoost - grid search for num_trees</summary><ul id="608b697a-c9e6-47e6-a0d5-6e428d283333" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test14.ini</strong></summary><figure id="7f6c8229-360d-44e7-b22f-209cf1404e91" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.05.32.png"><img style="width:514px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.05.32.png"/></a></figure></details></li></ul><ul id="e379e550-75f3-47c4-b7fc-6cc899734ca6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="e700d22d-f883-4b76-a1d8-c6f923fd6e5f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.08.46.png"><img style="width:816px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.08.46.png"/></a></figure></details></li></ul><ul id="742db531-7fec-4583-8178-ef525d341463" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="83a5107e-6d48-4707-bf96-1663d02f7cd3" class="">These results look odd; basically there is not difference when we add more trees. This suggests that the other params (max_depth 1, min_split 25) are having a big impact, so next we will look to change them and see if we get better results.</p><p id="aaa63f07-62da-4c8a-b721-913fcc5e539c" class="">
</p></details></li></ul></details></li></ul><ul id="914e3bf6-3b2c-481d-ae5b-397b0a13ece3" class="toggle"><li><details><summary>Test 15 - AdaBoost - grid search for max depth</summary><ul id="423d9833-d6ba-4b69-85be-0cedbfdcc133" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test15.ini</strong></summary><figure id="7edf504e-c73b-4bef-b602-d13e8e13b315" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.16.17.png"><img style="width:560px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.16.17.png"/></a></figure></details></li></ul><ul id="4693057a-f125-4bc7-8842-cef7f71cd91e" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ba41b11d-69a1-4924-97ca-cfeed4cfa989" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.19.32.png"><img style="width:912px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.19.32.png"/></a></figure></details></li></ul><ul id="0946d8ed-9024-4f82-a371-ed818e0311b6" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="9fca8056-2fa2-48e5-bbf7-45118449c8d6" class="">The results show that min split has limited impact, but we get bad results with the default max_depth of 1. Although AdaBoost is typically setup to use stumps it seems with this dataset it does not perform well with stumps. When we allow each tree to grow to 10 or deeper we get best results. The results above show a blob of white at max-depth = 3 and min_split = 3, so we&#x27;ll use those and do another search for the best number of trees.</p><p id="3d517228-2cfb-4f04-91a7-560ee96fcd49" class="">
</p></details></li></ul></details></li></ul><ul id="67a3c7e4-02f4-41fc-af48-7baf59388c19" class="toggle"><li><details><summary>Test 16 - AdaBoost - grid search for num_trees (again)</summary><ul id="5666d152-8fd5-4842-9a0e-82e89b4258a6" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test16.ini</strong></summary><figure id="7e609c07-1920-44bf-8408-f0f5c32bf136" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.25.09.png"><img style="width:518px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.25.09.png"/></a></figure></details></li></ul><ul id="0201859d-938f-4c54-9b68-c53eec7c29fa" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="201c5eef-1b8b-4c12-84cb-fc20a8415e08" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.23.33.png"><img style="width:1175px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.23.33.png"/></a></figure></details></li></ul><ul id="2b50ebfd-7a41-44b0-8f29-b9e7851ceddb" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="038d3e98-14b2-45fb-ae05-c389d0915a38" class="">The results show we are improving now with more trees and our improved settings for split and depth. The results are still going up a little beyond our grid search range, so the next test will take it on from there and see if we get more improvements by increasing the num of trees beyond 45 </p><p id="27aa1746-3e77-4959-97aa-0975d2a4dbb6" class="">
</p></details></li></ul></details></li></ul><ul id="79ecf4ac-29c4-4732-8d37-61ef4e0af391" class="toggle"><li><details><summary>Test 17 - AdaBoost - extended grid search for num_trees</summary><ul id="b443506d-2f41-43eb-9c8b-4ac37814c0f9" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test17.ini</strong></summary><figure id="9192bb75-15a2-40a0-9a7f-53db76cf0303" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.41.14.png"><img style="width:509px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.41.14.png"/></a></figure></details></li></ul><ul id="f9313f99-4baf-4719-936a-ecd16cbc86a5" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="c90612cd-3d4b-4f08-bbc1-c2ba24d26a00" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.40.46.png"><img style="width:960px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.40.46.png"/></a></figure></details></li></ul><ul id="40c6dffc-e901-4632-981f-fa58b46da4a0" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="9018442e-51cd-4a4c-9370-be1ce8d7258c" class="">Looks like we get improvements up to about 100 trees and then things start to decline. 90 seems the best , so lets see what we get against test data with those settings.... </p><p id="84171a94-7fe5-458c-b7f8-fcbc6c4efae7" class="">
</p></details></li></ul></details></li></ul><ul id="9b0a4cec-4116-4ba6-b9ee-0f0fd7c07bb4" class="toggle"><li><details><summary>Test 18 - AdaBoost - best params</summary><ul id="7f972cb1-1d40-4435-9a2c-5a8b9ecdbf36" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test13.ini</strong></summary><figure id="746ddd11-0b8f-4115-9fa2-f66dfe81c18b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.45.30.png"><img style="width:315px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.45.30.png"/></a></figure></details></li></ul><ul id="5e927bab-82a1-4096-a067-1e71f53d0be7" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="3a587382-3d15-49ad-bee1-07af54f7c98f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.45.02.png"><img style="width:816px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_17.45.02.png"/></a></figure></details></li></ul><ul id="cea9f9a8-1732-4a61-a50f-1da6b0e95ba0" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="52e8a0e6-6315-4c53-8fcb-10d84afdbc36" class="">These results are much better than our first attempt with an impressive 97.55% accuracy, which is the best we have had (although it has to be said not that much more than our basic DT in this case.) The other difference here is that like the baseline DT this model is using all 6 features , but doing so fairly equally, compared ot the baseline DT, and same for Bagging, where although all features were used there were 3 more prominent features - looks like AdaBoost is working hard to get the most outof each feature and gets a slight boost for that.</p><p id="7cd43ab7-b5ea-4341-88e1-9716b7bffa11" class="">
</p></details></li></ul></details></li></ul><p id="fee38cad-9032-4f71-831c-8bdc808fc092" class="">
</p><h2 id="1dc2f631-26e3-4852-830e-605804aaf223" class="">3.5 Ensemble 3a - Boosting (AdaBoost - amended)</h2><p id="d32a3b5f-95e7-4cea-a480-c45da6dd14d9" class="">With test 14 above, when looking to find the best num_trees, it seemed like our model was not fitting at all. There is an alternative implementation (or rather a slight amendment) designed to work better for multi-class datasets such as this one. This set of tests will repeat the previous set but using this alternative implementation to see if there are any differences.</p><ul id="5d4df52a-1b06-409b-8e06-98a34df11c8c" class="toggle"><li><details><summary>Test 19 - AdaBoost Amended - baseline using default parameters</summary><ul id="941fd972-c48a-4fd2-80c1-ee3d75ebf32e" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test19.ini</strong></summary><figure id="83be8263-3e72-4309-b7f2-c01b7ba11c05" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_09.47.58.png"><img style="width:536px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_09.47.58.png"/></a></figure></details></li></ul><ul id="bc0c65ce-b475-4477-936d-2e6169d107c9" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="f63ee46b-8f41-49fd-a4d2-84f43de3e99f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_09.48.39.png"><img style="width:864px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_09.48.39.png"/></a></figure></details></li></ul><ul id="4daf2ab3-7bf7-40df-b76c-a867ad10f83c" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f1865f93-834b-4949-8f71-67faabce5760" class="">This version of AdaBoost did not perform as well as test 13, down to 67% from 71%, but this is just using some arbitrary settings.</p></details></li></ul></details></li></ul><ul id="e1d26bf1-042e-4edb-bd23-6bdb536218f8" class="toggle"><li><details><summary>Test 20 - AdaBoost Amended - grid search for num_trees</summary><ul id="a9e90dd3-23ff-43c8-b408-90b364d012ab" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test20.ini</strong></summary><figure id="6af9dbb2-0f8b-4315-8919-e80cef866222" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.24.43.png"><img style="width:524px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.24.43.png"/></a></figure></details></li></ul><ul id="424e7cf8-ea41-433e-bac5-3b63db4c13bc" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="8051a21e-268b-416c-966c-ec1e127761e0" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.21.28.png"><img style="width:960px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.21.28.png"/></a></figure></details></li></ul><ul id="15b32b63-98e3-49f5-967a-8790f52f25fc" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="159c40dc-e758-4b8b-97fe-4a4f834ec021" class="">This is a dramatic difference to test 14, which essentially has a flat line with number of trees not making any difference. The results show that adding more trees to the ensemble is still adding value after 100, so the next test will look for results beyond 100</p><p id="8747816f-393d-4980-bf75-7084b8313c91" class="">
</p></details></li></ul></details></li></ul><ul id="1d895d77-3538-43c6-8507-249258e1ce8c" class="toggle"><li><details><summary>Test 21 - AdaBoost Amended - grid search for num_trees (extended)</summary><ul id="e0422661-6abb-4c9b-ac25-b42ff0d26ecf" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test21.ini</strong></summary><figure id="3de51057-1597-4aff-823e-0f4dd470a1d6" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.26.26.png"><img style="width:634px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.26.26.png"/></a></figure></details></li></ul><ul id="622f530a-1e4e-4017-a14d-36f7b5b6d35e" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="15960eee-db02-4022-b4da-1b02ecf650f0" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.34.20.png"><img style="width:912px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.34.20.png"/></a></figure></details></li></ul><ul id="62b03afa-4ca3-4f00-9e65-f8c3006e4228" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="32d09432-10db-4f8c-9d32-5f7ea2e4c866" class="">This seems to still be gaining by adding new trees, which is much more than the previous AdaBoost tests. We need another extension to see what happens after 1400</p><p id="15ef5455-3ec2-450d-837b-2cdc9145db4f" class="">
</p></details></li></ul></details></li></ul><ul id="29718eda-f9d6-4d0e-b0f8-226d07c3b446" class="toggle"><li><details><summary>Test 22 - AdaBoost Amended - grid search for num_trees (extended a bit more )</summary><ul id="229f9763-81c4-4592-8adf-fb8eb2df4bec" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test22.ini</strong></summary><figure id="ceefa98b-281a-4321-9c99-fa90cef2d4da" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.55.05.png"><img style="width:655px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.55.05.png"/></a></figure></details></li></ul><ul id="bda174b9-aea9-49bf-9787-a1efcca7ced6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="a6aeaf7f-8836-492c-bb05-49ea0f55366f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.52.42.png"><img style="width:1169px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_10.52.42.png"/></a></figure></details></li></ul><ul id="7ffaca8c-af49-413a-88f8-15ef54bd0b8d" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="5abd461f-0a12-4a0d-bcbb-0d6feea10d74" class="">The best results were at 1200 trees. The graph looks dramatic, but the scale is all over such a tiny range that its not far from a flat line overall. We&#x27;ll take 1200 as our best number of trees for this model. now we need to find the best min_split value</p><p id="78513689-a5e4-4e1c-9330-2dc1c19e3d08" class="">
</p></details></li></ul><p id="16852dad-7a1a-49b9-8acb-f00a1f5d3e99" class="">
</p><p id="fac87345-15f4-4cac-806f-03f80730b7f9" class="">
</p><p id="82e04d5c-13bc-45b1-bd66-b1bbd836fc35" class="">
</p><p id="eddaa8f1-4558-45c1-bc83-d5d2026103fa" class="">
</p></details></li></ul><ul id="d28fd80b-bfbb-41dd-a7f5-c31508f6f9d1" class="toggle"><li><details><summary>Test 23 - AdaBoost Amended - grid search for min_split</summary><ul id="6b6c437a-5869-4fae-b707-1eb68de00c49" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test23.ini</strong></summary><figure id="18dc49b3-4859-404a-9e7f-9d25c9c41612" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_11.34.39.png"><img style="width:425px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_11.34.39.png"/></a></figure></details></li></ul><ul id="47fb6334-b133-478a-b456-0b0b077a90c5" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="6770825c-31d7-4585-b47e-93c9ca79a8c4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_11.50.54.png"><img style="width:960px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_11.50.54.png"/></a></figure></details></li></ul><ul id="8a1ff949-eba0-483f-8fc0-c9ce85f44004" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="1ecc075b-99d7-4b9f-a48a-1a7d44b7c3c4" class="">There is no difference for the min_split so we will leave it at 1</p></details></li></ul></details></li></ul><ul id="9a0a10be-975f-438a-a11a-9050b7fa6536" class="toggle"><li><details><summary>Test 24 - AdaBoost Amended -  grid search for max_depth</summary><ul id="1d9e24e8-8fa8-4e8c-a2b9-72caa924d87e" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test24.ini</strong></summary><figure id="0b8cf4b5-9c4e-4c92-87eb-95b00bce6367" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.33.10.png"><img style="width:463px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.33.10.png"/></a></figure></details></li></ul><ul id="caffe729-6453-4812-b0fd-37c91f22f93a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="32745a2f-15c8-40ab-aa0f-04278f71f42f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.32.23.png"><img style="width:912px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.32.23.png"/></a></figure></details></li></ul><ul id="3c051473-c377-4aa5-b05a-a4c85d978cc9" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="0db3301b-7b66-42cb-9ca9-c5a4ef75b177" class="">Anything past 5 gives us 97% CV score. Things seem to steady off after about 10. Going to pick 10 for this value.</p><p id="12bd0d49-d1be-4096-b905-fd22c42a67a5" class="">
</p></details></li></ul></details></li></ul><ul id="9f9c0b63-f7ff-43b4-bd83-b37a9e899e35" class="toggle"><li><details><summary>Test 25 - AdaBoost Amended -  best params</summary><ul id="3be4e45a-9507-4509-8132-1dcfad33f0c6" class="toggle"><li><details><summary><strong>Config: ./config/classification/cars/test25.ini</strong></summary><figure id="f0421bab-f205-44d8-8edb-7f295e480cd4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.55.59.png"><img style="width:440px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.55.59.png"/></a></figure></details></li></ul><ul id="7770751e-8dbc-463c-9aae-a6fb87033bb9" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="3a53910b-51d5-4923-a76a-ea7f4800fc8e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.56.20.png"><img style="width:1183px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_14.56.20.png"/></a></figure></details></li></ul><ul id="3b5e3abc-df57-4690-970d-2685721d1136" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="cffba478-c4d1-4ab5-ba77-e3e351a2bb67" class="">These results, although arrived at using a different algorithm, and different hyperparameters, are very similar to the other AdaBoost results at 97.55%. </p><p id="7bb299d9-d98b-4a28-ab39-cd350ffc59fb" class="">This model was slightly easier to train since it responded well to trying different numbers of trees in the ensemble. It also uses a lot more trees (1200 compared to 90). However, with careful tuing the base AdaBoost algorithm did perform just as well as this multi-class version. That may not be the true of other datasets though, where this multi-set version may do better.</p><p id="5c930db6-89a0-4e39-b91f-e3296c5f03e9" class="">
</p></details></li></ul><p id="34d6abbd-261d-4d5f-9e5f-56fa7b3d8b43" class="">
</p><p id="d3eaf834-6024-4be9-beba-84807fb962b0" class="">
</p></details></li></ul><h1 id="48f9d725-5512-4197-8e8c-66a74b969877" class="">4. Regression - Housing Dataset</h1><h2 id="1e43f78c-c264-441c-a47b-658f9d265b6f" class="">4.1 BaseLine Decision Tree</h2><ul id="d5afff01-2633-4f84-b9c7-f786f5b3a76e" class="toggle"><li><details><summary>Test1 - baseline regression with defaults </summary><ul id="08dc4cef-4eb3-4d93-abc8-eb4de6d3899d" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test1.ini</strong></summary><figure id="81be2ddd-71d8-4568-a4a5-9b995b806f1a" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_18.38.40.png"><img style="width:535px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-12_at_18.38.40.png"/></a></figure></details></li></ul><ul id="bf72c446-5988-4eb7-9692-61dd936eff00" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="5d2e6e66-8fff-41df-aa84-7c45b7c9cfb2" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.06.06.png"><img style="width:1156px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.06.06.png"/></a></figure></details></li></ul><ul id="c70f75f4-63a0-42b1-99ea-4b993425781e" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="a4838df6-4b5f-4be9-8aa3-bb5b6c57a7d5" class="">The default decision tree, grown fully with no pruning does not seem to fully overfit the training data , with only 90%. This may suggest that adjusting some paramters may change these results. </p><p id="7394628f-b8a7-4c35-9ff8-b62df9e2abe2" class="">Despite having 13 features it only used 4 features. This is similar to the classification case (wine data).</p><p id="b90a2543-1756-4cc6-8a6a-5a09cebd931a" class="">The other thing that is very noticeable here is that the feature importance is very biased towards a single feature (LSTAT). We will look into this in more detail in other tests.</p><p id="a668d624-684d-461d-b469-106b7ca8ffd0" class="">No hyperparamters were set, but in the settings in the report we see :</p><ul id="34a927e7-9bfe-49c8-ba06-378fa8f77d37" class="bulleted-list"><li>min_split = 25</li></ul><ul id="ef25c22e-36bd-4bb2-adf7-b5259003e5f3" class="bulleted-list"><li>max_depth = 1000</li></ul><p id="dd973232-7b71-455f-91aa-0e80f0d1eba3" class="">These are code defaults; i.e when not set in config these fall back to the hard coded defaults for the model.</p><p id="b49db100-e22c-4e5a-97bd-c0aaa4e60c2b" class="">There are not many hyperparams we can tune for our base model. The only 2 parameters are as above. </p><p id="c50aa49c-ab18-490e-a16f-0d0885562cea" class="">What happens for our baseline tree if we amend the min_split and max_depth values? </p><p id="f8a78c2e-8fde-45ce-9cfa-9b4fa38b6eca" class="">
</p></details></li></ul></details></li></ul><ul id="165b810e-bd25-4ee6-84a4-a33c57d820dd" class="toggle"><li><details><summary>Test 2 - grid search for min_split</summary><ul id="fc6b4563-e1be-4bf0-badb-3148b2c2dd3f" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test2.ini</strong></summary><figure id="a9d193d1-0399-42b9-8b5d-1afb0256ad87" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.49.10.png"><img style="width:590px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.49.10.png"/></a></figure></details></li></ul><ul id="0f7587c3-ef26-404b-931f-6d9a54c79b00" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="749b7964-76bc-4997-8730-da19b3a82b00" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.07.02.png"><img style="width:1151px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.07.02.png"/></a></figure></details></li></ul><ul id="d09af7e2-1818-4846-9376-fb229050a506" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f61531c2-4952-4d35-afe9-5186dfdfa686" class="">There is improvement when num_split is less than 25 (the default), with best performance around the 8-10 range which yields a CV score of 80%</p><p id="c1266426-0e9e-42d5-bef2-7c91c14c2600" class="">We will choose to set this at 8 and see how this impacts out test set in the next test.</p><p id="982e1235-f7fc-4279-825e-b5c5f8ca4792" class="">
</p></details></li></ul><p id="76326569-c43f-4709-8d62-9ff56c67f86b" class="">
</p></details></li></ul><ul id="a5026b28-ee68-41ff-8754-e35b0d7ff9c3" class="toggle"><li><details><summary>Test 3 - baseline regression with tuned min_split</summary><ul id="3556c41b-74e4-4ea2-9e35-bd739b68655a" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test3.ini</strong></summary><figure id="80b3593a-3403-498e-8cec-9c29da1fe838" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.50.20.png"><img style="width:538px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.50.20.png"/></a></figure></details></li></ul><ul id="3db0492a-a22c-431e-adf9-2ad5da294ffc" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="a3a9c05f-d748-495d-9228-f4d243ce6419" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.07.31.png"><img style="width:1153px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.07.31.png"/></a></figure></details></li></ul><ul id="65101ed4-e488-4f68-b008-527f580404ca" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="cb550b1f-57fa-4073-b168-d7fcbe71855c" class="">Test performance has increased from to 82% to 87% . Now we have a better setting for min_split we will test to see what params to use for other params e.g max_depth</p><p id="1602f8cf-b2ad-450f-9a4c-ffd9c0a83221" class="">
</p></details></li></ul><p id="d6fc3a12-8845-450f-ad3c-25c2327be159" class="">
</p></details></li></ul><ul id="bf711183-c420-45c7-a3ac-0f6a96e8ceed" class="toggle"><li><details><summary>Test 4 - grid search for max_depth</summary><ul id="f8feade9-46fa-4811-beba-e9017693e254" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test4.ini</strong></summary><figure id="b73fa4bd-1a09-40c5-877c-54337e820e44" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.51.15.png"><img style="width:595px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.51.15.png"/></a></figure></details></li></ul><ul id="ae45fab7-4d8c-42db-a99f-8ace63c70b00" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="de80fbc8-5f28-411b-9e4f-eba536c60174" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.07.56.png"><img style="width:1164px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.07.56.png"/></a></figure></details></li></ul><ul id="0925aea2-b626-4cb8-9dfa-580e1843690b" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="eff84fbc-8950-4042-afaa-ea1be538ae1e" class="bulleted-list"><li>There is little difference with setting the max_depth; any value above 5 gives us the best results, with no difference going beyond 10. This is probably due to the very dominant single feature meaning we are ending up with shallow trees.</li></ul><ul id="69bb57d5-2b45-41ec-ae83-da32a57490d4" class="bulleted-list"><li>We will set this to be 10</li></ul><p id="aa6401f0-2296-4499-ab61-beb9441d35d8" class="">
</p></details></li></ul></details></li></ul><ul id="8fec58e4-0da7-4df9-b044-061941e7884d" class="toggle"><li><details><summary>Test 5 - baseline regression with tuned min_split and max_depth</summary><ul id="6941cfe7-8bf5-4009-bd75-ec40a15b9bab" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test5.ini</strong></summary><figure id="fd992463-d639-456b-926c-2a5573e8c287" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_16.16.54.png"><img style="width:597px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_16.16.54.png"/></a></figure><p id="6cad4553-01b4-4882-b8aa-aeca4b61cef9" class="">
</p></details></li></ul><ul id="c1f9ccab-a75c-476a-84e0-896cf2b90fc2" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="78eb190b-3352-4ef1-a9c8-b36343bb5dab" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.08.17.png"><img style="width:1152px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.08.17.png"/></a></figure></details></li></ul><ul id="7942e4cd-c2aa-4702-a39e-5f4017c1a949" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="3b33cf52-bdac-48a0-bec4-636215f2769a" class="">This last run did yield a very slight increase in test performance from 87.24 to 87.25%! I think we are at the max performance for this decision tree. Time to start looking at some ensemble variations to get better results....</p></details></li></ul></details></li></ul><h2 id="cd23ba42-a105-4493-8eeb-d414c6c16cd2" class="">4.2 Ensemble 1 - Bagging </h2><p id="67cf1ce0-d51c-448f-a944-56d0f0b2a985" class="">Moving on from the base Decision Tree, now looking at extending this to use Bagging instead, with multiple simple decision trees used</p><ul id="1c756200-b032-42ff-bff0-0c1ca6e4593f" class="toggle"><li><details><summary>Test6 - bagging with (mostly) default parameters</summary><p id="b9cac10e-c116-4082-9d5a-7dace4eef0c2" class="">For bagging we do need to specify at least a setting for the number of trees (number of base models) to add into our ensemble. </p><p id="e980731c-c78b-4358-9a51-2e905048de3a" class="">The other Hyper Parameters for bagging are the same as for the base Decision Tree.</p><p id="8197b1d3-3540-4124-920b-8a0d9c7e7734" class="">This test will keep all the defaults for the base tree settings, and choose an arbitary number of tress as 5 to get a baseline to work with.</p><ul id="b9dcc376-ac5b-40ed-a28e-8af1db777f5c" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test6.ini</strong></summary><figure id="c63e9295-8826-4573-bd03-4ac78e22f254" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.51.56.png"><img style="width:472px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.51.56.png"/></a></figure></details></li></ul><ul id="173cc288-291a-4e61-aec8-5a413d769962" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="c803f332-98c2-4540-9651-6bccfa4d857e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.08.45.png"><img style="width:1153px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.08.45.png"/></a></figure></details></li></ul><ul id="58203435-5190-4ac0-b719-e01c253f2c36" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f3851a6d-e2f9-4af6-ba8a-7e7fa93f4e64" class="">This ensemble does not perform as well as the base DT with only 5 trees in the ensemble, reaching only 92% on training and 81% for test data. </p><p id="d220016f-2379-4149-a325-756d66471049" class="">What is also interesting is that the feature importances are also very different. For the base tree LSTAT was very much the most important , whereas here we see the previoulsy second placed RM being the most important feature. </p><p id="b75d4c47-1af3-4b20-b318-58aebf8f89b6" class="">However, this could be the reason why the results are not as good as the baseline DT! </p></details></li></ul><p id="ea2bdb88-cd64-4812-85f4-abd3f5ee9b47" class="">
</p></details></li></ul><ul id="a66bd610-df44-4ece-a346-b1e39e05eb11" class="toggle"><li><details><summary>Test7 - bagging - grid search to find num_trees</summary><ul id="da76d9f2-42a3-45c5-b4e9-0c8760652b50" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test7.ini</strong></summary><p id="3f2e9fd4-a884-434d-9bcc-e44b2347e6aa" class="">The list of values tested here starts with our poor first guess of 5 and goes up to 45. </p><figure id="5ee6030a-9008-44d3-b354-705dc54f79d4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.52.15.png"><img style="width:408px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.52.15.png"/></a></figure><p id="b18155bf-5ea4-492b-bdc6-0d36e6bc34e7" class="">
</p></details></li></ul><ul id="c76b2b73-185e-4447-926c-76441ab84510" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="d2434232-70ad-43a7-8a60-bb6d91c4fe65" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.09.13.png"><img style="width:1154px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.09.13.png"/></a></figure></details></li></ul><ul id="d590e75b-53ae-491f-9df4-afaf6bba9572" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="95aa42c0-bd38-4c4f-bb54-f5f6be04c6f0" class="">Looks like there is benefit in increasing the number of trees at least up to 35. Set to 35 and run another test.</p></details></li></ul></details></li></ul><ul id="48ddc1a6-1596-47fd-ba23-e97b9e40546f" class="toggle"><li><details><summary>Test 8 - bagging - tuned num_trees</summary><ul id="bbd258c0-564a-4bd1-a902-93eb0b13b26f" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test8.ini</strong></summary><figure id="346fd0be-de0d-4457-9a33-8925b081628b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.52.33.png"><img style="width:425px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.52.33.png"/></a></figure><p id="fd1a98d4-6fa2-46a4-87e3-1b5674d9bf27" class="">
</p></details></li></ul><ul id="f22684a8-8360-4c64-b42f-2a26bd3e3983" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="fced1ce7-2c65-42ad-925d-c20cb7155ad4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.09.40.png"><img style="width:1151px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.09.40.png"/></a></figure></details></li></ul><ul id="6874628a-913a-4467-893a-5f821bd66f64" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="4af4dfdd-3e9f-4945-bfd0-ef863953e208" class="">Changed from 81% to just below 86%. We have tuned the num_trees, now lets look at min_split and max_depth together</p></details></li></ul><p id="bde0924c-120a-4e47-abef-de85e166c68a" class="">
</p></details></li></ul><ul id="6c167555-f593-4312-adc1-82703cc0809d" class="toggle"><li><details><summary>Test 9 - bagging - grid search for both min_split and max_depth best values </summary><ul id="ce931714-e26e-4d84-8afd-a5c6d9f28b66" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test9.ini</strong></summary><figure id="a7d334cd-c952-4df7-8a39-c9d5b32658b3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.52.51.png"><img style="width:551px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.52.51.png"/></a></figure><p id="67ff4add-fa81-465b-8c99-c969f179a3f0" class="">
</p></details></li></ul><ul id="4ca9622b-ba4c-46b3-b745-416471e6dcc1" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="499254ae-7ca5-4d0b-a342-7df6cda2ddad" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.10.11.png"><img style="width:1144px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.10.11.png"/></a></figure></details></li></ul><ul id="9a0c8aa8-cc86-49ba-9256-30e716e5768b" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="4ff8efc8-b0dd-400b-8282-b7bb381475b4" class="bulleted-list"><li>The results suggest the best settings are to have min_split &lt; 10 and max_depth &gt; 4.</li></ul><ul id="95e32b8d-4f3f-490e-a721-d90ab4797f76" class="bulleted-list"><li>We will go with min_split=2 and max_depth=10</li></ul><ul id="dbbb8fad-3069-438d-9b81-26b019fa3b0a" class="bulleted-list"><li>Now we have tuned parameters, we can re-run our test and see how results have improved from the previous high of 85.98%</li></ul></details></li></ul></details></li></ul><ul id="4d6a09b2-c568-49a6-81c3-90463ad286ec" class="toggle"><li><details><summary>Test 10 - bagging - with tuned parameters</summary><ul id="ed80bf96-2a42-4a96-8d4c-e074cf892266" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test10.ini</strong></summary><figure id="459c5eff-90ee-42cf-9723-43f06c138af4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.53.47.png"><img style="width:311px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.53.47.png"/></a></figure><p id="adf6f5ad-76a7-49e5-8cb2-8b58e3249c57" class="">
</p></details></li></ul><ul id="a77d2365-8682-4913-9006-47b856d9e558" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="e0db4f02-eea8-42e3-8d93-6443040910bb" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.11.50.png"><img style="width:1157px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.11.50.png"/></a></figure></details></li></ul><ul id="531909ef-30a0-4815-b246-d2a95244c42f" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="6a9a8ff3-b24a-486a-9df5-a52007786917" class="">The results improved from 85.98% to 86.75%  </p><p id="4051ddd5-722e-450a-b4df-f6a21d3e1bac" class="">This is a much better score than we started with, but the usage of features still looks like most features are not used very much.</p><p id="a1cb66d3-b247-4262-aafd-05b2aa6c9958" class="">What would happen if we used the same model settings but removed the bottom ranked features?</p></details></li></ul></details></li></ul><ul id="e18a29bd-f9ff-44e0-bf1b-73ac4a2cc873" class="toggle"><li><details><summary>Test 11 - bagging - tuned params, trim bottom 5 features</summary><ul id="08345326-e9c5-4816-a758-b2e58a221384" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test11.ini</strong></summary><figure id="f86e0773-7f02-418c-bcb1-67825cad8583" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.02.png"><img style="width:520px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.02.png"/></a></figure></details></li></ul><ul id="f17bb922-caf2-461b-9520-42dbcd63e5c4" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="c11a2daa-d915-4910-ba04-cf5eec73ae6b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.12.24.png"><img style="width:1134px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.12.24.png"/></a></figure></details></li></ul><ul id="0eb87268-419e-4619-9540-92ede5998a8f" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="1f4fe91f-5990-4206-ae7a-de13b639d351" class="bulleted-list"><li>Improved slightly from 86.75% to 87.18%</li></ul><ul id="78a59451-b515-4149-905e-188067c10dff" class="bulleted-list"><li>Lets see what happens if we trim more ......</li></ul></details></li></ul></details></li></ul><ul id="8f983a37-7fee-41ba-9977-40acd9a1234c" class="toggle"><li><details><summary>Test 12 - bagging - tuned params, trim more features (just keep top 3)</summary><ul id="5a36bd79-3463-4b81-95b9-0d83cf7ad3f9" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test12.ini</strong></summary><figure id="0ff1b141-ffa5-47fa-ab2a-4861fa26dff9" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.15.png"><img style="width:575px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.15.png"/></a></figure></details></li></ul><ul id="41683663-6523-43c6-97f2-5ab922346990" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="4cf60b0c-07ad-46ef-862d-819cec5643e7" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.13.00.png"><img style="width:1147px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.13.00.png"/></a></figure></details></li></ul><ul id="c68fb597-272e-490b-97ee-f60fe0396cdf" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="7c6da77a-43ec-43f8-80ef-ec1f683eb43c" class="bulleted-list"><li>This did not improve results; although the lowest ranked features seem to not contribute much, trimming further reduces the overall score e.g just using the top 3, LSTAT, RM and DIS resulted in reduction to 82%.</li></ul></details></li></ul></details></li></ul><h2 id="d5603143-b3c7-4c72-8b63-dbff9fd1186a" class="">4.3 Ensemble 2 - Random Forest</h2><ul id="aa372d4b-b5aa-456f-b1db-2239b406b490" class="bulleted-list"><li>Repeating tests done for the bagging ensemble, but using the Random Forest approach instead. </li></ul><ul id="982322be-a0dc-4a7d-aa7d-d2cb8a94d146" class="bulleted-list"><li>For Random Forest the same base hyper parameters are applicable, but has the extra one of : num_features<ul id="628cb927-0500-4a99-96c7-f807d3219a98" class="bulleted-list"><li>This allows us to tune the model choosing how many (randomly chosen) features should be included for each base model. This is a mandatory new hyper parameter for the RF model</li></ul></li></ul><ul id="cec5cfad-145b-45fe-a4fd-6e6db64a895e" class="toggle"><li><details><summary>Test13 - Random Forest with (mostly) default parameters</summary><ul id="ea8af9bd-575d-45ea-92f0-9e0917853f51" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test13.ini</strong></summary><p id="424f1154-d39d-4a5a-8f23-afd01db49fe2" class="">As with other baseline tests will keep all the defaults for the base tree settings, and similarly choose an arbitrary number of tress (a mandatory parameter) as 5 .</p><p id="978382d3-b40c-4b01-b3b3-a35c3afe9129" class="">For the new parameter of num_features, since this data set has 13 features in total we will choose roughly half and set it to 7</p><figure id="1ddd521e-4a0b-499c-8558-dfd40921419b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.35.png"><img style="width:495px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.35.png"/></a></figure></details></li></ul><ul id="47a4d6b0-fcde-40cf-8a35-7b8c4d215079" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="e8050368-32ef-4aa7-96cf-83dbba01a533" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.13.28.png"><img style="width:1158px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.13.28.png"/></a></figure><p id="f799bce3-71e5-46ba-a482-22d8554b034f" class="">
</p></details></li></ul><ul id="a566ebc6-6988-4684-b2f0-cddd6c285ba2" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="cd65bbec-9553-467a-bbc9-b5a1fdf9047e" class="">At 79% test <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> this ensemble performs less well than the baseline DT(83%) and not even as good as the baseline Bagger (81%), and certainly nowhere near as well as the tuned Bagger  at 87%.</p><p id="36d71c19-d48d-45dd-86a8-7f61dc3243cf" class="">However, this is probably due to the hyper parameters being used. The feature importance has promoted some previosuly low ranked features up the rankings here too.</p><p id="8ca20673-f24f-4836-ad97-bf6fb1b063e6" class="">In the following tests we will look to tune settings for : num_trees, num_features, min_split, max_depth, </p></details></li></ul></details></li></ul><ul id="ae39a1b0-b84e-4137-8f8e-95f83ab9a5d1" class="toggle"><li><details><summary>Test14 - Random Forest - grid search to find num_trees</summary><ul id="4eb1f6d5-f8ac-4792-9baf-9d5e9539a59f" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test14.ini</strong></summary><p id="59b3b8d9-e66f-406f-b7b1-b6113e86d10f" class="">The list of values tested here starts with out poor first guess of 5 and goes up from there to see if more trees will improve the performance. </p><figure id="a724a411-4ecc-45d3-88dd-96016e6509af" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.52.png"><img style="width:662px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.54.52.png"/></a></figure></details></li></ul><ul id="52989532-0d11-49b9-94cb-0d655cc808ba" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="22df12e4-f7c0-4791-bb37-1fbf26fa0e2b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.13.54.png"><img style="width:1166px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.13.54.png"/></a></figure></details></li></ul><ul id="074c0759-7c40-4137-86a0-55e47180e455" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f9cdb348-1306-4646-ae08-c9fdab1f9c3e" class="">Like the Bagger the RF model sees improvements with more base models added. These results seem to suggest that we want at least 200 base models to get the best results. </p></details></li></ul></details></li></ul><ul id="9fea98a0-a3a8-480d-8c5b-eb3033e4911c" class="toggle"><li><details><summary>Test15 - Random Forest - with tuned num_trees (200)</summary><ul id="53aacc92-86c3-422a-84f3-b0b049b6c20f" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test15.ini</strong></summary><p id="103b9b9a-0343-487a-9641-be3fb4e41352" class="">Repeat basic test but with num_trees set to 200</p><figure id="bd33df01-7267-4b84-9b34-1b23ab08e919" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.55.09.png"><img style="width:450px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.55.09.png"/></a></figure></details></li></ul><ul id="7578cfa2-61b9-4df3-b313-ad435f10ba4a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="7f7fe639-127d-40d4-88a8-b39cd899c444" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.14.27.png"><img style="width:1162px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.14.27.png"/></a></figure></details></li></ul><ul id="90fca61e-2b6e-4795-ba22-2cee74411068" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="075369cd-2297-4552-a316-98a83798e993" class="">Results have improved from to 79% to 83%. Now need to tune other params too, for example num_features</p></details></li></ul></details></li></ul><ul id="40651c76-0d06-4289-b0b6-716290e2a372" class="toggle"><li><details><summary>Test16 - Random Forest - grid search for min_split and max_depth</summary><ul id="5d1dcd48-f7ba-466f-90ad-f6d95ad2023f" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test16.ini</strong></summary><p id="564210ab-bdcb-4e84-9604-a7dc2d4669b1" class="">Keeping the new value for num_trees at 200, this test will investigate the best value for min_split and max_depth </p><figure id="7be67b4a-3efa-4497-8b23-a1f14f2a1088" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.55.24.png"><img style="width:594px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_08.55.24.png"/></a></figure></details></li></ul><ul id="bd417265-9751-4f6d-9741-b35aac5a0dc2" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="a3e23af1-c044-410e-ae6b-0b273ab13cc5" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.14.56.png"><img style="width:1160px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.14.56.png"/></a></figure></details></li></ul><ul id="598c485e-0b16-40b0-9f73-670aba3687b7" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="413c9c07-1074-47d8-9134-64d33694579c" class="">lower min_splits are better(1), and max_depth is best at 15. </p></details></li></ul><p id="8530f238-aae2-47fb-94f5-bbaa3bddd4f1" class="">
</p><p id="093d7ee2-f9d4-41ce-abdb-8d4b4ad02dc9" class="">
</p></details></li></ul><ul id="049b8970-78a7-498f-9a4e-6450d8fa40cb" class="toggle"><li><details><summary>Test17 - Random Forest - grid search to find num_features</summary><ul id="035ae959-f6c9-4e40-a2e9-64770132e11e" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test17.ini</strong></summary><figure id="2e3ace1a-83c0-4264-9bd1-d4f6b6dab334" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_16.06.35.png"><img style="width:528px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-13_at_16.06.35.png"/></a></figure></details></li></ul><ul id="003d7a00-cca7-4876-b197-8dec1f908731" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="4bf535e2-18fe-4d41-8018-386405fb539b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.15.25.png"><img style="width:1153px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.15.25.png"/></a></figure></details></li></ul><ul id="aa1bfd10-cc83-485f-aa81-62a347d4e783" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="ca4d7149-ebff-49c9-93ac-ac4310272bc0" class="">Anything over 7 gets good results, with best at 11. One thing to note here though is that looking at the feature importance rankings one feature is very much more important that all the others, and in fact you can probably make a good decision tree with just 2 of the features. Where RF drops these 2 features the resulting trees are not going to do so well.</p></details></li></ul><p id="46a360c1-bcb5-490e-a5a7-743a14ff8f69" class="">
</p><p id="29c551c7-7914-4289-9dc5-436d1afa4cd5" class="">
</p><p id="6a2a916a-7bb8-4202-9d5c-39b0291be74e" class="">
</p></details></li></ul><ul id="32f5eac5-353b-4942-b6c0-bed946aaeaff" class="toggle"><li><details><summary>Test18 - Random Forest - with best parameters found so far</summary><ul id="d3c00cb7-7c6e-4259-bd87-da17eccb5974" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test18.ini</strong></summary><figure id="3c49a72c-c9be-48ef-9b7c-77740f03f10d" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.24.27.png"><img style="width:459px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.24.27.png"/></a></figure></details></li></ul><ul id="49cafc64-6fee-4566-ae66-4ea728d65094" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="f2618775-93c2-4948-b8ed-97b1b0601935" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.15.49.png"><img style="width:1157px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.15.49.png"/></a></figure></details></li></ul><ul id="e3e7d432-42d3-4590-bc65-34fad5c999c7" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="e47ee750-5e7e-437e-bd6d-f6950a669c96" class="">RF is only doing as well as Bagging here - one thing to note here though is that looking at the feature importance rankings one feature is very much more important that all the others, and in fact you can make a good decision tree with just 2 of the features; the baseline DT used only 4 to get a test <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> of 87%. Where RF randomly drops these 2 features the resulting trees are not going to do so well. This could be the reason why RF does not do so well here - it may do better where the important features are more evenly distributed. This is interesting as in the previous dataset (Cars) we could see RF not doing so well due to unbalqanced classes in the data, and now we see it not doing so well with unbalanced feature importance ratings. With either of therse 2 issues it seems to default back to being only as good as bagging (with decsion trees).</p><p id="a61b8179-56bd-47bb-b95e-d486eb88f6e1" class="">
</p></details></li></ul><p id="555c8a74-4cdb-4427-933c-a7f00304ca18" class="">
</p></details></li></ul><ul id="e6c03c43-8c0a-45a5-a6be-a99494a80024" class="toggle"><li><details><summary>Test19 - Random Forest - with less features - trim bottom 5</summary><p id="0ed51155-0ea0-4f49-8e27-0ff1fd25344e" class="">The idea here is to &#x27;rebalance&#x27; the features a bit by removing some of the &#x27;weaker&#x27; ones. We did the same for bagging (see Test 11 and 12) so can compare the impact on both.</p><ul id="2596035c-8442-48c9-92a4-c4f330bc0761" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test19.ini</strong></summary><figure id="4b79444c-044c-433f-b8d1-186debbdfcba" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.56.27.png"><img style="width:517px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.56.27.png"/></a></figure></details></li></ul><ul id="0b25af1b-cd16-4ba5-8f8c-4bd1cccd0feb" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="fcf24f69-ead4-4484-a8a3-4dc2099782d3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.57.14.png"><img style="width:1149px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_09.57.14.png"/></a></figure></details></li></ul><ul id="97e9c9ac-f631-4830-b9cc-176fd4feb8ee" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="88b33cef-9b7f-4cea-8e82-654477b91807" class="">Interestingly the score has gone up by using fewer of the less important features. This fits with the idea that what is causing RF a problem is reliance on the weaker feature, which normally would be fine except that in this case they are so unbalanced that has a big impact. This also does slightly better than bagging (test11), although only just.</p></details></li></ul></details></li></ul><ul id="853fd289-c238-4442-bdb9-d24bb3b1d882" class="toggle"><li><details><summary>Test20 - Random Forest - with less features - just top 3</summary><p id="54919cbf-0f15-4049-9c90-7fd88768cf8c" class="">The idea here is to only include really useful features to see what impact this has . </p><ul id="b089ddf1-7033-403b-a61d-0bf9325ab032" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test19.ini</strong></summary><figure id="a5d44fa5-96af-482c-b538-7f3f91eb1dcf" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.02.24.png"><img style="width:530px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.02.24.png"/></a></figure></details></li></ul><ul id="04e9b20d-81a8-46c6-90a6-12a32ec1ea83" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="648a03bd-b4a8-4d9e-abe4-0498cfe53923" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.02.43.png"><img style="width:1164px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.02.43.png"/></a></figure></details></li></ul><ul id="3aad46ac-7729-49c5-bd5d-34cc6a3d786a" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="111e801b-a6d0-436a-ad36-367d5a59ac24" class="">This does not perform as well, since even the &#x27;lesser&#x27; feature do contribute to the overall result, so trimming too far impacts the models ability to predict well. This does slightly outperform bagging, but only very slightly. The RF is caught in a difficult place where trimming the smaller features does mean that it has more chance of including the all (too) important top features, but going too far limits its ability to both get varied trees and fit well to the data. </p></details></li></ul></details></li></ul><h2 id="ff722bc7-9f46-4c25-8c8f-5117d9208fd8" class="">4.4 Ensemble 3 - Boosting (Gradient Boosting)</h2><ul id="7e9b0e73-c001-4563-a0c1-8470ddca1033" class="toggle"><li><details><summary>Test21 - Gradient Boost -baseline parameters</summary><p id="e3ce0cf8-0248-4a51-8667-0e033623ba7f" class="">The hyper parameters for Gradient Boost include all the parameters for the base Decision Tree, and also include a mandatory one for num_trees (similar to bagging and RF)</p><p id="b2c2aafc-4d94-4104-b335-8310b768fe5b" class="">This test will use an arbitrary number of tress (5) to get a baseline Gradient Boost model to work with.</p><ul id="d2c3e437-0e6d-42d6-a2c7-ddf8eaf48391" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test21.ini</strong></summary><figure id="6c24ada7-1dd3-4921-afee-e42109bdf20e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.18.43.png"><img style="width:390px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.18.43.png"/></a></figure></details></li></ul><ul id="1446fdbb-519c-436f-9f03-050e93823ec1" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="6730dea9-d02f-4d2d-84cc-416a15e69b65" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.19.03.png"><img style="width:1163px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.19.03.png"/></a></figure></details></li></ul><ul id="22851518-d8d5-4083-9ded-1b0d8045967f" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="e919d9a2-3a29-4180-a572-75dd6ef9481b" class="">These results are very poor with just over chance (53%) score. This is not too surprising since we would expect to have many more trees than 5 in a boosting ensemble. The next test will look to do a grid search on num_trees to improve these results.</p></details></li></ul><p id="be49c140-2d84-469b-8b01-af21422eae13" class="">
</p></details></li></ul><ul id="251da8db-3fde-42d3-9f92-e13015717eb8" class="toggle"><li><details><summary>Test22 - Gradient Boost -grid search for num_trees</summary><ul id="79b0d8e1-afde-4e2d-83a5-89926ce95dc4" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test22.ini</strong></summary><figure id="d9206f62-18ad-4b05-a24e-57c872234fbf" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.28.15.png"><img style="width:489px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_10.28.15.png"/></a></figure></details></li></ul><ul id="8a2d0a7a-6a28-4a24-b7aa-6b0bc8574dc2" class="toggle"><li><details><summary><strong>Results</strong></summary></details></li></ul><ul id="bcb04b01-e508-4f6e-af18-3c82c5405398" class="toggle"><li><details><summary><strong>Thoughts</strong></summary></details></li></ul><p id="fb21aac2-0d96-4131-bd1a-7cd336aa2af6" class="">
</p></details></li></ul><ul id="17fbb474-eeb4-4a7c-8e06-7f764e421d89" class="toggle"><li><details><summary>Test23 - Gradient Boost -grid search for num_trees (extended)</summary><ul id="775afa98-f3dc-4570-9f78-a91131a421a3" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test23.ini</strong></summary><figure id="85b46a05-b595-4fe0-85c8-f5e2b96773f1" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_11.26.54.png"><img style="width:575px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_11.26.54.png"/></a></figure></details></li></ul><ul id="f4bf07f7-0474-4c48-9a89-2d10683300e1" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="445a27d3-7d72-44ab-810b-a41cd2e5537e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_12.42.38.png"><img style="width:1165px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_12.42.38.png"/></a></figure></details></li></ul><ul id="5150503b-41e5-49ec-864c-71369f29bb1a" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="ad9cb726-3ada-4dae-8d61-c2308f6c351c" class="">Improving scores up to at least 1000. The numbers are quite close from there to 1400, so another grid search beyond 1400 is called for .... </p></details></li></ul><p id="57f5d90d-536f-4f83-9fd7-990ad42d7515" class="">
</p></details></li></ul><ul id="f91bf72b-38ed-424d-ac18-1ffa97d1b598" class="toggle"><li><details><summary>Test24 - Gradient Boost -grid search for num_trees (extended again)</summary><ul id="0e7a8976-3120-42ec-9524-0928b8c8cbe1" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test24.ini</strong></summary><figure id="3b6a64b5-2bbc-411c-b63b-c349bef02090" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_12.44.21.png"><img style="width:610px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_12.44.21.png"/></a></figure></details></li></ul><ul id="5bb3a9ae-a787-423b-abc7-109ba7ae0b71" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="cf928eff-2124-4e61-a0c3-c37bd00a588e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_13.22.31.png"><img style="width:1158px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_13.22.31.png"/></a></figure></details></li></ul><ul id="754db61b-f7ea-4f5a-a0c8-7629dcbef1f5" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="4caeb28b-ba95-426d-8635-6bc4611584ca" class="">Quite conclusive that score is going down after 1400, so best score is between 1000 and 1400.</p></details></li></ul></details></li></ul><ul id="2e9f01e1-fc3d-4749-a69a-16a3b12e9923" class="toggle"><li><details><summary>Test25 - Gradient Boost - best params</summary><ul id="353abb18-4428-46a7-b889-2765a699f76a" class="toggle"><li><details><summary><strong>Config: ./config/regression/housing/test25.ini</strong></summary><figure id="b4f1ac3c-5584-4d30-b24d-4aa11ed4a7e3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_14.02.10.png"><img style="width:364px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-14_at_14.02.10.png"/></a></figure></details></li></ul><ul id="aa8227f5-d3ea-4287-b56f-9dc2d10f532c" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="674dea9c-477e-4300-9135-8dce5575f3c9" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-10_at_10.59.49.png"><img style="width:1171px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-10_at_10.59.49.png"/></a></figure></details></li></ul><ul id="8bc2b125-ea2e-47b5-a6b0-a80e34851ca1" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="89711e02-61db-4b27-8f67-e11b6b9da426" class="">The performance here is the best we have seen on this dataset. Gradient boosting is not as impacted by the unbalanced feature importance it would seem, although these results are not too dissimilar to the base DT either, so overall the benefits of the ensembles are reduced by this factor in the dataset.</p></details></li></ul></details></li></ul><h1 id="17c3f931-2d9a-4a63-a100-48f56da03ac1" class="">5. Regression - Concrete Dataset</h1><h2 id="6447c3bb-57ab-4a3b-a1bf-c26767a5c96b" class="">5.1 BaseLine Decision Tree</h2><ul id="fbf5dc75-4c24-4730-a853-181c74bc1003" class="toggle"><li><details><summary>Test1 - baseline regression with defaults </summary><ul id="a8856ab9-1d70-48c9-82af-99822ca05880" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test1.ini</strong></summary><figure id="f46d0fb4-52fa-433d-9096-bdad2e5515bf" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.00.png"><img style="width:526px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.00.png"/></a></figure></details></li></ul><ul id="1fb30bb2-9a1b-4263-999b-3529f5e40a32" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="401693ec-46d6-4506-9b8e-4cbd248ef6e1" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.17.55.png"><img style="width:1154px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.17.55.png"/></a></figure></details></li></ul><ul id="5e550317-c1e1-4e29-b154-32d5ba609d4e" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="91e8cb59-6deb-4acb-a694-beb9bf4dad19" class="">Training <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> is 87.90, and test <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> is 75.45%. It only used 6 out of 8 features, and 3 of those to minor effect. one feature seems to be very dominant (Age). </p><p id="9d5b953a-212c-4928-9262-4e1874bded3a" class="">
</p></details></li></ul></details></li></ul><ul id="7080df1f-98ca-4e39-a842-9fd013e76e03" class="toggle"><li><details><summary>Test 2 - grid search for min_split</summary><ul id="14630322-8dd8-4690-b23e-dafad812d624" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test2.ini</strong></summary><figure id="2c4e9b83-8bd9-4c45-a766-d24ce63f896e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.09.png"><img style="width:595px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.09.png"/></a></figure></details></li></ul><ul id="ac68cd0e-ff51-498b-ad7f-c9b3b601384c" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="77b3ea1f-0e30-463f-a02a-2a7b3cc6b0aa" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.59.46.png"><img style="width:1168px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.59.46.png"/></a></figure></details></li></ul><ul id="b15dfb1e-e2e2-44c8-b489-e5eb04781d2e" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="02a66415-b341-4667-b454-80aa84acfa72" class="">Having a low min_split as possible is improving results. We will set this to 1 for now. </p></details></li></ul><p id="d8fc0f5b-99b9-4b22-9fca-7eac68faed96" class="">
</p></details></li></ul><ul id="8f41f5b4-f165-4865-82e4-31cede00877b" class="toggle"><li><details><summary>Test 3 - baseline regression with tuned min_split</summary><ul id="2f6f69aa-5375-40f9-ad9d-23f716b3ad16" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test3.ini</strong></summary><figure id="5c15c87b-d7fa-41d4-a4dd-c95d9a8e544a" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.45.png"><img style="width:468px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.45.png"/></a></figure></details></li></ul><ul id="f0ffddd6-52d4-4918-9e92-d87ec62dc81d" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="bad441bc-147a-42ab-86af-e7447847c1aa" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.18.22.png"><img style="width:1153px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.18.22.png"/></a></figure></details></li></ul><ul id="241049ff-9751-4ea8-966e-9e9567f479eb" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="5eec5bd4-e692-4c92-9f11-b4912c0c886a" class="">Test performance has increased from to 75% to 80% . Now we have a better setting for min_split we will test to see what params to use for other params e.g max_depth</p><p id="5eb00a34-0648-4968-8d42-2855244b2051" class="">
</p></details></li></ul><p id="08db6f3a-5d0f-4cbf-82a3-83d83d280e67" class="">
</p></details></li></ul><ul id="c00a4060-ef3b-40b2-b3ff-f17ec7e8d365" class="toggle"><li><details><summary>Test 4 - grid search for max_depth</summary><ul id="5892b551-013f-4520-b56e-fd634cbc1f27" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test4.ini</strong></summary><figure id="4c8ed081-1f42-450e-b8c5-2238df9a67b8" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.59.png"><img style="width:631px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.51.59.png"/></a></figure></details></li></ul><ul id="7648a370-cb5d-4344-81bb-41b064bba451" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="67912790-090a-43d0-bf11-1432a0f3bfa4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.02.30.png"><img style="width:1171px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.02.30.png"/></a></figure></details></li></ul><ul id="2f2671f5-fee2-4574-aecb-04c3b8d411bd" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="085a48e6-3741-40a5-a473-59b9781756b2" class="bulleted-list"><li>Anything over a depth of 20 is giving us the top scores here; we&#x27;ll set this to 20. </li></ul><p id="0eaea24b-526b-4de5-abde-e2ff2d0e2902" class="">
</p></details></li></ul></details></li></ul><ul id="31f3ad8b-30f0-4784-9c13-40c856b81ce9" class="toggle"><li><details><summary>Test 5 - baseline regression with tuned min_split and max_depth</summary><ul id="a474971a-8780-4f60-a39e-4b7b0710261e" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test5.ini</strong></summary><figure id="5bfb49cd-51b3-42f5-a816-b80f504421cd" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.09.png"><img style="width:639px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.09.png"/></a></figure><p id="3f8286a6-1d86-477c-87b7-0b9b7c60b570" class="">
</p></details></li></ul><ul id="efc02543-b513-485a-adcf-a46de3c00d0f" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="775f8d48-3d50-4cdd-bfe8-de6207d3a8e8" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.18.54.png"><img style="width:1153px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.18.54.png"/></a></figure></details></li></ul><ul id="5dd61712-6519-4738-9b42-73182661e212" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="6a44cef9-dd2a-40d2-b9e5-b39ded1e9e62" class="">Out final scores for the baseline DT is 80.34% test <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> . This is a reasonably good improvement on our staring point, but lets see if the ensemble models can improve on this. </p></details></li></ul></details></li></ul><h2 id="6a6765a5-1fd6-4526-a77d-197721bff1d4" class="">5.2 Ensemble 1 - Bagging </h2><p id="57be30c2-e92e-4a51-82fc-972e7b848d00" class="">Moving on from the base Decision Tree, now looking at extending this to use Bagging instead, with multiple simple decision trees used</p><ul id="e29f5798-489b-40ff-8d6e-209458933b7f" class="toggle"><li><details><summary>Test6 - bagging with (mostly) default parameters</summary><p id="1df0892b-b7f0-464e-87fc-de8c28a41b3d" class="">For bagging we do need to specify at least a setting for the number of trees (number of base models) to add into our ensemble. </p><p id="0858a602-c92e-4808-854d-d27c0f974944" class="">The other Hyper Parameters for bagging are the same as for the base Decision Tree.</p><p id="6605f382-1894-4a45-8fc4-0a6bae3a853d" class="">This test will keep all the defaults for the base tree settings, and choose an arbitary number of tress as 5 to get a baseline to work with.</p><ul id="ddbfe57f-02b3-4d16-8c95-8267bf6986c8" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test6.ini</strong></summary><figure id="a19a90e6-b93e-4647-be43-9fcf648d1359" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.17.png"><img style="width:505px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.17.png"/></a></figure></details></li></ul><ul id="92abcb86-f661-4e67-92c4-20eed0c0d6fe" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="4fd20f1e-d715-48f6-a5a7-6fec0338c5ab" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.05.54.png"><img style="width:1167px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.05.54.png"/></a></figure></details></li></ul><ul id="49c31fbb-e47b-4f41-b25d-dcd363b2010b" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="4ce02fec-403c-4be8-acf3-2225b401e697" class="">This ensemble already outperforms the basleine DT with a test score of 85% (compared to baseline of 80%). </p><p id="b38a9889-ffe0-4f70-84a5-2b7cf3d15e6c" class="">Now we will tune this to be even better. </p></details></li></ul><p id="21ca514b-c23e-4485-8745-432fe00a5cdd" class="">
</p></details></li></ul><ul id="e73b4325-572d-4994-9800-b0309c662429" class="toggle"><li><details><summary>Test7 - bagging - grid search to find num_trees</summary><ul id="9002062e-d0c3-4607-b457-b1b23a9472e3" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test7.ini</strong></summary><p id="f0755937-0ea1-46a5-afaf-91d73abc6266" class="">The list of values tested here starts with our poor first guess of 5 and goes up to 45. </p><figure id="7fe01a06-04e4-4901-a484-8682f77c43e3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.38.png"><img style="width:433px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.38.png"/></a></figure><p id="86d909c3-1a5c-4c12-8727-91ab998e455a" class="">
</p></details></li></ul><ul id="0ebdc4fe-e47c-4594-b0f3-605b26497710" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="f91c214b-9708-4c13-9f55-79255a5f1a58" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.07.59.png"><img style="width:1171px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.07.59.png"/></a></figure></details></li></ul><ul id="78836c88-ede6-4f4e-baf2-8e110bd85723" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="edb0cfb2-61fe-4a10-bdfe-214c7b1fbe0b" class="">Looks like there is benefit in increasing the number of trees at least up to 35. Set to 35 and run another test.</p></details></li></ul></details></li></ul><ul id="f0ca8c3a-f7c9-4667-8f36-63391f966765" class="toggle"><li><details><summary>Test 8 - bagging - tuned num_trees</summary><ul id="5070a528-e05c-4133-a732-fe6f0da5b434" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test8.ini</strong></summary><figure id="b336de63-527c-4c1e-a742-10f52ba31a7d" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.54.png"><img style="width:407px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.52.54.png"/></a></figure><p id="1e9f36e4-800c-4b38-864b-d081b0592413" class="">
</p></details></li></ul><ul id="027eba81-db39-4808-b62a-20fac8c75aef" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="43f481b1-c779-4351-931f-8cadc1b60ce2" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.08.59.png"><img style="width:1181px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.08.59.png"/></a></figure></details></li></ul><ul id="51f98341-9c85-4a3c-97ba-67e7c3664068" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="8bd0b6ff-6f19-4012-95c4-b9a7f0bfe593" class="">Adding a few more trees (up to 35) has increased the test score from 85% to 87%, a significant improvement. </p><p id="76b00e82-e4fe-4efa-92b6-10b9c61f5f65" class="">Now we will look to set the other parameters.</p></details></li></ul><p id="e2acf222-7c97-4972-b469-d633e84d5b93" class="">
</p></details></li></ul><ul id="e7c45f27-884b-4b77-931e-db388a55e9da" class="toggle"><li><details><summary>Test 9 - bagging - grid search for both min_split and max_depth best values </summary><ul id="2f40a8d3-91c2-42cd-8339-e36e6148596a" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test9.ini</strong></summary><figure id="05c8c950-e85c-4925-8895-1db419fc9c66" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.53.21.png"><img style="width:624px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.53.21.png"/></a></figure><p id="3288cc1b-882d-4a44-ac21-68369df3aeb6" class="">
</p></details></li></ul><ul id="445baf00-8930-4bfc-bf84-b5f380faf434" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ac02c509-e522-4809-ab46-09b5061eb57d" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.10.34.png"><img style="width:1168px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.10.34.png"/></a></figure></details></li></ul><ul id="d360f769-6830-4a53-b8d6-167a89320b26" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="96b361e4-fc25-4d04-89b3-5322df859d3a" class="bulleted-list"><li>The results suggest the best settings are to have min_split = 2 (or possibly 1) </li></ul><ul id="0afb7da3-e0a0-45b1-905b-d2e7e5d47a18" class="bulleted-list"><li>It is less clear for the max_depth setting : the next test will drill into this in more detail.</li></ul></details></li></ul></details></li></ul><ul id="3d703825-98cd-4a51-82b0-74c21762f274" class="toggle"><li><details><summary>Test 10 - bagging - grid search for max_depth (again) </summary><ul id="13c377f2-ba76-421d-ade7-2c1faf26e899" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test10.ini</strong></summary><figure id="dbd7119d-8bef-46c9-b4c1-84be2ff70865" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.54.04.png"><img style="width:577px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.54.04.png"/></a></figure><p id="80d26d32-4b48-4757-a917-7d37b76d50e9" class="">
</p></details></li></ul><ul id="d9f60981-6d70-4f72-8b29-7171e042e64c" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="eda366a1-c7b2-4074-b3cb-1feec7b73a80" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.13.33.png"><img style="width:1185px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.13.33.png"/></a></figure></details></li></ul><ul id="d8ec204a-16fa-4fec-bd49-7333a1f68890" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="03c14f4b-1c40-4a00-a89c-0c91cc4847b8" class="">Increasing the max_depth improves performance up to about 25. Set this to 25. </p><p id="46f5618d-19e3-4036-995f-36a84b986d43" class="">
</p></details></li></ul></details></li></ul><ul id="4f9a0323-276c-429d-b18c-0f22254bed01" class="toggle"><li><details><summary>Test 11 - bagging - tuned params</summary><ul id="2f22d8be-fb1f-4d47-bc4a-cfbcc12292b3" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test11.ini</strong></summary><figure id="6318fb36-4a88-4f43-8a5e-4f9ae89532d5" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.54.22.png"><img style="width:333px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_17.54.22.png"/></a></figure></details></li></ul><ul id="348b0a4c-c7be-40c1-818c-d02b49c2154d" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="5f9e27b7-ee9c-4136-8d15-8e3baabe456d" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.14.47.png"><img style="width:1174px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.14.47.png"/></a></figure><p id="376c7b89-9730-4915-a8d0-3f926ca121ca" class="">
</p></details></li></ul><ul id="a12df500-b40c-4d8b-952e-46e2ff9eaf52" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><ul id="82cfd9c4-d15b-41fc-afd5-0b169633ff2e" class="bulleted-list"><li>Performance has now improved a lot from a starting point of 85% to 90.90%</li></ul><ul id="b738b006-3096-4b46-ae6e-0e09bab1f615" class="bulleted-list"><li>This has performed much better than the base Decision Tree</li></ul></details></li></ul></details></li></ul><h2 id="a372f901-c511-4f04-9a27-ab41d39d06a9" class="">5.3 Ensemble 2 - Random Forest</h2><ul id="55a995cf-975e-4234-8100-9b8e00d1b586" class="bulleted-list"><li>Repeating tests done for the bagging ensemble, but using the Random Forest approach instead. </li></ul><ul id="c51eb250-f835-4449-ad78-3a2f7a59c8e0" class="bulleted-list"><li>For Random Forest the same base hyper parameters are applicable, but has the extra one of : num_features<ul id="7ed3fa36-e3ce-4a84-ba7c-bfb071e4b75d" class="bulleted-list"><li>This allows us to tune the model choosing how many (randomly chosen) features should be included for each base model. This is a mandatory new hyper parameter for the RF model</li></ul></li></ul><ul id="dad7e3cd-fff4-4ea1-8042-4621b792a951" class="toggle"><li><details><summary>Test12 - Random Forest with (mostly) default parameters</summary><ul id="b2817342-8ee0-46b2-bb14-1c4f50f215f7" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test12.ini</strong></summary><p id="579e79a9-b14a-4259-b843-e9742460ee1f" class="">As with other baseline tests will keep all the defaults for the base tree settings, and similarly choose an arbitrary number of tress (a mandatory parameter) as 5 .</p><p id="c163eb09-10ce-487d-8f6c-fb7a658903c0" class="">For the new parameter of num_features, since this data set has 8 features in total we will choose roughly half and set it to 4</p><figure id="4ae17e5f-4f8a-42f1-bea0-7827f4ffc831" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-18_at_14.27.59.png"><img style="width:507px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-18_at_14.27.59.png"/></a></figure></details></li></ul><ul id="24214e36-2042-40ac-bb82-59d836ade2fa" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="60afc7e9-d20a-4c60-8020-467473e7b701" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.19.07.png"><img style="width:1166px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.19.07.png"/></a></figure><p id="09307f7f-0873-4b8d-b0ce-1802517b273a" class="">
</p></details></li></ul><ul id="3bd55ebc-57a1-40af-a95e-894fbc68765e" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="71a2de84-ea78-48b1-8a3a-e3b79ace9acf" class="">At 54% test <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> this ensemble performance is weak compared to the baseline DT(80% tuned) and not even as good as the baseline Bagger (85%), and certainly nowhere near as well as the tuned Bagger (90.9%).</p><p id="0701ce29-30c4-4b05-9364-b3845b7110b8" class="">In the following tests we will look to tune settings for the hyperparameters to see if we can get this model to perform better.  </p></details></li></ul></details></li></ul><ul id="19af7c56-0481-4748-9f66-a5dee07b088a" class="toggle"><li><details><summary>Test13 - Random Forest - grid search to find num_trees</summary><ul id="9139d2e4-8a4e-42c1-8a39-af81a454e53e" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test13.ini</strong></summary><p id="7a9bbdc4-eb80-4c20-803b-524adbe8a950" class="">The list of values tested here starts with out poor first guess of 5 and goes up from there to see if more trees will improve the performance. </p><figure id="4d6c3486-d023-45b0-9520-d423c060d261" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.22.11.png"><img style="width:684px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.22.11.png"/></a></figure></details></li></ul><ul id="d6bc9ed7-e715-4856-a245-dedd0fe5b486" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="ffa01fa1-1998-4492-ac3b-8cf4f5aa975b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.22.29.png"><img style="width:1172px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.22.29.png"/></a></figure></details></li></ul><ul id="8fe960f1-2e0b-4979-a5cb-c0dd93d584c2" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="0b8498a2-8823-4aa4-aab7-318fa2d7f858" class="">Like the Bagger the RF model sees improvements with more base models added. These results seem to suggest that we want at least 120 base models to get the best results. </p></details></li></ul></details></li></ul><ul id="ecb9cafe-a4dd-4ae4-8774-a5240e513d87" class="toggle"><li><details><summary>Test14 - Random Forest - with tuned num_trees (120)</summary><ul id="d84bb49c-9634-4b85-8bef-fc1137e9bcd0" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test14.ini</strong></summary><p id="5e775b18-1faa-47d9-97c7-930e3c5be62c" class="">Repeat basic test but with num_trees set to 120</p><figure id="e86eca89-01bb-49aa-9d1f-8b3706a3ac7f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.23.10.png"><img style="width:464px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.23.10.png"/></a></figure></details></li></ul><ul id="c44062f7-1085-4418-b95c-750b8ea159b6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="e955c940-eb1e-496f-8c24-1fd6d7e17465" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.23.27.png"><img style="width:1173px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.23.27.png"/></a></figure></details></li></ul><ul id="33bf230b-17f4-4cb9-96bd-c7490d06cd78" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="e5745172-2874-4ef1-b28b-260492085937" class="">This did improve performance a lot (moving from 54% to 78%),  but this is still behind the Bagging results.  Now need to tune other params too, for example min_split and max_depth, and num_features</p></details></li></ul></details></li></ul><ul id="7e063cc3-1925-4aae-9fa5-f1e2abf79ac6" class="toggle"><li><details><summary>Test15 - Random Forest - grid search for min_split and max_depth</summary><ul id="b26b37d2-915e-49f8-9312-c6dfb38080b4" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test15.ini</strong></summary><p id="beebbc31-3420-4041-a832-e712942f0b33" class="">Keeping the new value for num_trees at 120, this test will investigate the best value for min_split and max_depth </p><figure id="6a809636-4c30-4496-9bb8-cfd9620d136c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.26.11.png"><img style="width:617px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.26.11.png"/></a></figure></details></li></ul><ul id="6dd9b670-ac98-48b6-bea3-81470305cfcd" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="e0d478ec-f5bf-4d5f-9e3b-72346fcf0099" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.26.40.png"><img style="width:1177px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.26.40.png"/></a></figure></details></li></ul><ul id="2d479c84-3c34-4d09-9b65-08ebd9a8d526" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="018fc505-5153-4e9d-9b8c-3e1ed9ca743d" class="">The best results are with min_split low (1) and max_depth at 25. There could be a case for doing another search on max_depth as its not in the middle of the grid, but we&#x27;ll set this to 25 for now and see if we should change it later.</p><p id="72a8935f-6760-4871-9ce7-4ef4d40d3fc4" class="">
</p></details></li></ul></details></li></ul><ul id="a6536eb2-26bf-451e-82bc-5c74f1e4c5fd" class="toggle"><li><details><summary>Test16 - Random Forest - grid search to find num_features</summary><ul id="ef8246a6-4b37-4e81-8d95-9aeebc23f4cd" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test16.ini</strong></summary><figure id="9e6435a2-51e9-4e48-a854-9fd1dfae74c9" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.28.41.png"><img style="width:534px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.28.41.png"/></a></figure><p id="3dd6cd74-65d2-4520-8317-594ae2aeebc5" class="">
</p></details></li></ul><ul id="6ae2d07f-7fd7-416d-8481-dfd9a2d8645a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="78ccbbc1-5a70-4e03-8e02-4e981a9e8e1f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.29.07.png"><img style="width:1171px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.29.07.png"/></a></figure></details></li></ul><ul id="8e43f23f-0c49-45f6-a0a4-3f6b5a6dd46c" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="86509790-14ce-4cf8-8543-fd19a2cf3e98" class="">This is not a good result for the Random Forest model. The result suggests that the best results are occuring when we use all 8 of the features, which means this degrades to a Bagging model i.e there will be no improvment over bagging here at all ! </p><p id="6b10fa0b-b3ed-4945-b338-5b4be9638abf" class="">Just to see if that is true we will run one more test to try RF with the best settings except to set the number of features to 7, as that is quite close in the above grid search results, and perhaps test will give different results.</p></details></li></ul><p id="1020c809-cef5-4e83-ae9a-b8f0b544334d" class="">
</p></details></li></ul><ul id="703117fb-b4e9-4753-ad3c-78605e572668" class="toggle"><li><details><summary>Test17 - Random Forest - with best parameters found so far (but use 7 features)</summary><ul id="a018713c-9ef6-4ee1-9350-0bcd59fffb67" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test17.ini</strong></summary><figure id="35db5779-ac20-444f-9754-fe0625b5f509" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.31.42.png"><img style="width:419px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.31.42.png"/></a></figure></details></li></ul><ul id="fe08f65c-a2d8-438c-a721-e43c9f43ca7d" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="1a2499fc-6cde-4ffb-9a73-1a2d849a7f12" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.32.03.png"><img style="width:1176px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.32.03.png"/></a></figure></details></li></ul><ul id="34e53d5b-c373-459d-b8e3-421784b37d13" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="782d3f33-1b25-42b5-9a82-010558e357af" class="">RF is only doing as well as (or slightly less) the Bagging ensemble here - again, similar to the housing data set, we do have 2 very dominant features in the mix here which may well be causing RF to not pick very useful trees when these are not part of the mix. </p><p id="c28f79db-e0d0-494c-8fff-baf5a70532f4" class="">
</p></details></li></ul></details></li></ul><h2 id="642416c7-e06f-4fc1-b684-dae92c5b1069" class="">5.4 Ensemble 3 - Boosting (Gradient Boosting)</h2><ul id="bddbb6c3-9a39-4394-b2ae-0ebb564c9109" class="toggle"><li><details><summary>Test18 - Gradient Boost -baseline parameters</summary><p id="95da8def-72dd-427b-915d-c40ca272517b" class="">The hyper parameters for Gradient Boost include all the parameters for the base Decision Tree, and also include a mandatory one for num_trees (similar to bagging and RF)</p><p id="7c9c36d9-8f10-485e-bb8f-8d1a232992fe" class="">This test will use an arbitrary number of tress (5) to get a baseline Gradient Boost model to work with.</p><ul id="aa6f95e4-0606-4255-900e-109e535561a1" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test18.ini</strong></summary><figure id="8d822bdb-e185-4769-96e4-07ebe6b49cc3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.38.50.png"><img style="width:391px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.38.50.png"/></a></figure></details></li></ul><ul id="913692a6-0812-45d5-ace0-52a10e31cc2a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="9e19d107-7e43-4a5f-9518-d6ad9a99ce51" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.40.26.png"><img style="width:1169px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.40.26.png"/></a></figure></details></li></ul><ul id="426e2ea0-c08e-4afe-8449-d43a79aab684" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="291173fb-3a92-44a3-a789-2c572cc5f50e" class="">The results here are as bad as the Random Forest baseline at just over 50%. </p></details></li></ul><p id="d6996c7c-78dd-4aae-806a-119871fc5088" class="">
</p></details></li></ul><ul id="bf6ff420-32b2-48ae-9d7e-2a8a8d340692" class="toggle"><li><details><summary>Test19 - Gradient Boost -grid search for num_trees</summary><ul id="3545e3bd-d6fd-47cc-8f3f-72ee067d9367" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test19.ini</strong></summary><figure id="321660c5-a053-4b6d-8e61-cf1692256afb" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.38.59.png"><img style="width:480px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.38.59.png"/></a></figure></details></li></ul><ul id="a100b1e7-877a-4b98-8937-7dad0e649881" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="3590f158-e949-40a2-a72a-e474f4d9bba7" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.43.54.png"><img style="width:1172px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.43.54.png"/></a></figure></details></li></ul><ul id="e44ce455-df94-45fa-af9c-fb0c3966ced2" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="6d1a1be5-84f0-496c-ae59-4fd379cd5acb" class="">increasing number of trees improves performance with 125 looking like a sweet spot. It could be that more trees may actually improve things even more. Set this to 125 for now, and come back to it.</p></details></li></ul><p id="5850264e-117f-4571-8fd9-eb1da4ac7490" class="">
</p></details></li></ul><ul id="b668754b-ff40-4abd-9d8f-dbfaf641c167" class="toggle"><li><details><summary>Test20 - Gradient Boost -tuned trees </summary><ul id="2d63dcf6-550e-424b-8b8a-b30308e28a3a" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test20.ini</strong></summary><figure id="643b23d4-0e30-45a9-a356-e6ddf7bafd2f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.11.png"><img style="width:356px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.11.png"/></a></figure></details></li></ul><ul id="04cfc4a7-2383-41dc-97e8-e24381e827fe" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="70f74dab-25bb-44af-87af-71d57d73d6e0" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.52.26.png"><img style="width:1162px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.52.26.png"/></a></figure></details></li></ul><ul id="c0737b2e-944b-4a2c-ba9b-65e3d6400c1a" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="85bedcad-1f15-40b4-9567-0492817b0bdb" class="">An amazing turn around in performance here. the increase in number of trees has driven the test performance from a low 50% to an impressive 91.49%.</p></details></li></ul><p id="be769a26-9506-4cce-a5b9-04cd7e8dc188" class="">
</p></details></li></ul><ul id="c8bcdb27-3947-4ecc-b080-c34b030cb071" class="toggle"><li><details><summary>Test21 - Gradient Boost -grid search for min_split</summary><ul id="7af2ae8a-1b02-4923-b8b4-3675e7609950" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test21.ini</strong></summary><figure id="c5a799c8-c5bf-47e6-930d-ecb702a1dcaa" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.23.png"><img style="width:584px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.23.png"/></a></figure></details></li></ul><ul id="e5aba01c-3f36-4e0a-8d17-3ea1fe2b3dc3" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="9252ad22-64a5-4976-8003-505c1038bbf3" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_19.39.42.png"><img style="width:1167px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_19.39.42.png"/></a></figure><p id="0ebd7b4f-63a8-4fdd-8e72-6856b9d5ea90" class="">
</p></details></li></ul><ul id="8bd1c2b4-7de8-4696-b316-ff16ffe649cb" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="d2f0c671-5920-4ee2-a00c-46423088e921" class="">Best values are above 25, best at 70, although results are similar after 70. Set this to 90 for now.</p></details></li></ul></details></li></ul><ul id="0d61d83c-a779-4b18-8166-cba43872dbe3" class="toggle"><li><details><summary>Test22 - Gradient Boost - best params, but look for num_trees again</summary><ul id="fc892167-3d7f-4fed-93ff-9f1fd1c646f3" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test25.ini</strong></summary><figure id="8e6d05d0-ceb4-4a2a-a0f8-ba363a633c3b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.47.png"><img style="width:643px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.47.png"/></a></figure></details></li></ul><ul id="3c008e40-f33f-49a5-825c-3925f64eb1e0" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="fa9b49a9-8659-462b-bb34-868261d43d3a" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.02.11.png"><img style="width:1170px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.02.11.png"/></a></figure><p id="e65581c4-a5b3-46c8-8141-a67e21b6acea" class="">
</p></details></li></ul><ul id="d0fd8741-947b-4394-af82-7177145e9812" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="8c5d23bf-afd9-4992-bc78-4bf0541b48b4" class="">We need a lot more trees to get the best performance here, best seen at 500.</p></details></li></ul><p id="0fe6ada0-057c-45be-995d-2413d0591144" class="">
</p></details></li></ul><ul id="52d11f54-910c-436c-b664-b0924839b8a6" class="toggle"><li><details><summary>Test23 - Gradient Boost - best params, </summary><ul id="a62856b7-dc2c-4f52-9b77-66c64dd2d759" class="toggle"><li><details><summary><strong>Config: ./config/regression/concrete/test23.ini</strong></summary><figure id="afc203de-4c35-4f14-9a9d-1f730eeef5f1" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.57.png"><img style="width:382px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_18.39.57.png"/></a></figure></details></li></ul><ul id="0eca460d-1866-4314-b2cc-35744c69b076" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="c6cc52aa-08a8-4308-b09e-f3b7c23b39ef" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.16.11.png"><img style="width:1162px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-08-17_at_21.16.11.png"/></a></figure><p id="7152fd96-40e4-4473-81b2-0a1495271917" class="">
</p></details></li></ul><ul id="d744f95e-b697-4f43-a006-87512ba0325e" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="fcb9c5ac-6ee8-447e-922a-ae96b6ce4a48" class="">The performance here is the best we have seen on this dataset at 94% - Gradient Boost is the best model for this dataset. </p></details></li></ul><p id="3a9b3d01-b729-4ab0-a6c3-8076508c3bf1" class="">
</p><p id="d4bc8aff-ae5a-4f96-b650-ef6dedbecb92" class="">
</p></details></li></ul><h1 id="f9632e1d-acf5-4db4-983e-018e0d3d8f05" class="">6. Cross Model Analysis </h1><p id="0566f258-a260-44c3-981d-b6ac075229dc" class="">This section will investigate how the different models react to the different hyperparameters used to tune them, for each of the datasets.</p><p id="3effbb81-f596-4a04-8f2d-8179c09114ae" class="">The settings used will be the &#x27;best&#x27; settings found by the previous experiments, except for the single &#x27;moving&#x27; parameter which will override any model specific settings and will be shown on the x-axis of the graphs.</p><h2 id="90f73491-1bfe-45fe-a3e6-5e3c62a55a59" class="">6.1 How does &#x27;num_trees&#x27; impact model performance?</h2><ul id="ae79b13d-ee5d-47d7-9912-fae3ab42db1d" class="toggle"><li><details><summary>Test 1 - Wine (all models)</summary><ul id="335a11c1-a931-48f7-8e52-5996ad436af2" class="toggle"><li><details><summary><strong>Config: ./config/x_model/num_trees/test1.ini</strong></summary><figure id="9640bdef-dce8-4cb8-a4fe-1aa005f085fa" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.35.27.png"><img style="width:491px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.35.27.png"/></a></figure><p id="189e6730-2a49-40b7-affe-f93c8c0e7275" class="">
</p></details></li></ul><ul id="ff1dc057-8c29-4eea-9f95-a92905f82ec6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="dadf6c99-bc7a-4aa7-a2bb-f914a210b233" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.48.06.png"><img style="width:1168px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.48.06.png"/></a></figure><p id="ef6c7f20-16af-4531-be24-55d8a5dd9971" class="">
</p></details></li></ul><ul id="173e8582-8aba-43e7-968b-7becbc4228f9" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f8378949-6c0d-44f1-b26f-5100e83bf9ce" class="">This result is a little too busy to really read, so another couple of tests will be run to split out some of the models.</p></details></li></ul></details></li></ul><ul id="0e4e98f1-3374-4a0f-b241-804bb71036da" class="toggle"><li><details><summary>Test 2 - Wine (RF, BOOST)</summary><ul id="885281a6-0e1b-4eb6-bc43-6a2f491f4f7a" class="toggle"><li><details><summary><strong>Config: ./config/x_model/num_trees/test2.ini</strong></summary><figure id="b704de92-0b3e-4a04-9b73-3554c67fe8e1" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.02.png"><img style="width:583px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.02.png"/></a></figure></details></li></ul><ul id="2782e5c1-5118-4adf-b972-32f938576b98" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="22bea8e2-4bcb-42a3-b8d8-036bc526fbd2" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.49.05.png"><img style="width:1166px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.49.05.png"/></a></figure><p id="fe6e557f-f95c-4af2-908a-906f22ac977f" class="">
</p></details></li></ul><ul id="615507ee-a70a-4ca2-8e49-b9cba564cec7" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="f698065f-1f75-4d3b-943b-ceffd8fc5f3c" class="">Both models improve with more trees, with RF taking a bit longer to peak compared to BOOST. The RF out-of-bag results seem better than test results with less trees, although they later get much closer. The RF results are consistently better than the BOOST results.</p></details></li></ul></details></li></ul><ul id="06b84fd6-7825-4c7d-84fe-b55d3416d688" class="toggle"><li><details><summary>Test 3 - Wine (BAG, RF)</summary><ul id="7d0b7da2-3026-4e5c-94d1-687df05b9af5" class="toggle"><li><details><summary><strong>Config: ./config/x_model/num_trees/test3.ini</strong></summary><figure id="13a4b70d-3acd-459e-857a-33c395b7cd4e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.22.png"><img style="width:572px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.22.png"/></a></figure></details></li></ul><ul id="083bae5b-0de8-4772-bb78-233ddedfef3b" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="fceb7f17-c063-409b-a76f-1eac6e6329c7" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.49.32.png"><img style="width:1160px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.49.32.png"/></a></figure><p id="905ba865-dd06-473f-986d-c50bef4990eb" class="">
</p></details></li></ul><ul id="19529b82-67b4-4cbf-a1b8-1cf86722bbe1" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="271f75c7-91c5-4153-b162-39ed4f739b1e" class="">Again both models respond well to more trees, with RF requiring more than BAG to reach its peak performance. </p></details></li></ul></details></li></ul><ul id="619d4ed6-3df3-4e25-9eec-7552adc8ef4b" class="toggle"><li><details><summary>Test 4 - Cars (all except RF)</summary><p id="a051a90b-c57e-4005-ba9a-f495cd11b443" class="">(note, RF was removed since it was only as good as bagging in this case)</p><ul id="dac11b67-614d-4982-b32c-fe505c87388b" class="toggle"><li><details><summary><strong>Config: ./config/x_model/num_trees/test4.ini</strong></summary><figure id="cad4bb18-0053-4750-b6b3-3c51be34466b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.38.png"><img style="width:534px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.38.png"/></a></figure></details></li></ul><ul id="3a7de4ca-6440-4307-afa0-327fb7cc2a8e" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="7b79b8c9-e4df-4c9f-aef1-e416564cc96b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.49.56.png"><img style="width:1160px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.49.56.png"/></a></figure><p id="a99b7bb8-e406-47f2-9b58-ecb3018a451f" class="">
</p></details></li></ul><ul id="553a4315-adec-47c9-98bb-73c2ac8492ea" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="3d50f305-6fb0-476b-8b34-9c35ffa345e8" class="">There is clear difference in overall performance between the Bagger and Booster here, with Boosting having consistently better results. The Bagger does respond to more trees, but this has little difference after about 100 trees, whereas for Boosting (boost_test, brown line) increasing the number of trees is having a significant impact the whole time. (The OOB for bagging consistently overestimated the real test results, but does follow a very similar pattern.)</p></details></li></ul></details></li></ul><ul id="e34be870-a6af-4324-aba3-7f7078094dae" class="toggle"><li><details><summary>Test 5 - Housing (all models)</summary><ul id="ee96d9b4-f7d1-4e9d-9acd-0b826da33ffd" class="toggle"><li><details><summary><strong>Config: ./config/x_model/num_trees/test5.ini</strong></summary><figure id="4bf04845-9c6a-44f7-924d-69be70f35e0a" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.52.png"><img style="width:530px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.36.52.png"/></a></figure></details></li></ul><ul id="3f2e042c-d045-4eb4-abaa-ac99a6bbf4c0" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="582c3138-8290-484f-928f-92fa8d3ddbd5" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.50.19.png"><img style="width:1165px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.50.19.png"/></a></figure><p id="9d99c023-ac00-49af-b8c9-f4f80e25ef9f" class="">
</p></details></li></ul><ul id="7472dd6b-3cdf-41b5-b044-cadf93d8dcc8" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="dcbd5539-90d2-4b18-9617-fb69372bb434" class="">All models have reached a steady state well before the 500 trees on offer here. Boosting takes slighlty more trees to reach its top results compared to the others.</p></details></li></ul></details></li></ul><ul id="cb39ba95-6234-440d-967b-5f6761130a0b" class="toggle"><li><details><summary>Test 6 - Concrete (all except RF)</summary><p id="f9ae6f18-7b3b-471c-b932-dec7b1559b64" class="">(note, RF was removed since it was only as good as bagging in this case)</p><ul id="b98a0a32-53cd-48fd-b1f6-87b411f1671b" class="toggle"><li><details><summary><strong>Config: ./config/x_model/num_trees/test6.ini</strong></summary><figure id="506fa298-5d81-4ed8-92a2-6eff13a4547c" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.37.08.png"><img style="width:522px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.37.08.png"/></a></figure></details></li></ul><ul id="eed99b10-fcdb-4877-adc8-ed0778790e2f" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="f766be24-75ae-4114-9d23-b4be099e8765" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.50.44.png"><img style="width:1163px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.50.44.png"/></a></figure><p id="e3267fa8-1420-4593-bd4d-4b27ef79a861" class="">
</p></details></li></ul><ul id="328faa1c-d8b3-4965-a152-3ff8dd51b65c" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="38df3fbe-c7aa-4f89-8772-a60d05b37caf" class="">Boosting taking more trees to reach top results again here.</p></details></li></ul></details></li></ul><ul id="637f14a9-f3b6-42d8-be21-4c1476783b95" class="toggle"><li><details><summary>Conclusions on impact of num_trees</summary><ul id="fbfd0ac0-1522-4929-8c3c-dafae4d02046" class="bulleted-list"><li>all generally respond well to more trees up to a point, </li></ul><ul id="dcff7721-dba5-4937-b7d0-d4eaaf94a94b" class="bulleted-list"><li>rf takes longer to steady out than bagging </li></ul><ul id="bdd4ead0-b6a7-4651-8cd0-404cdba7d2f7" class="bulleted-list"><li>boosting seems to need more trees than the rest and seems to get a more gradual and steady response to having more trees<ul id="c5e94f21-f06a-45b0-a198-3afbc968fc4c" class="bulleted-list"><li>in particular see test 4 to see this, although in general boosting takes more trees to reach its peak than others.</li></ul></li></ul></details></li></ul><h2 id="3149dca9-8063-49f5-b343-0bbf1da7b5a8" class="">6.2 How does &#x27;max_depth&#x27; impact model performance?</h2><ul id="6328e68a-1b1c-41ce-bb44-72bd8c74873c" class="toggle"><li><details><summary>Test 1 - Wine (all models)</summary><ul id="cb7423d8-7446-40fe-b950-2c07960ea71f" class="toggle"><li><details><summary><strong>Config: ./config/x_model/max_depth/test1.ini</strong></summary><figure id="5f3ced79-db0b-4a6e-95a7-f13a8555983b" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.37.35.png"><img style="width:498px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.37.35.png"/></a></figure></details></li></ul><ul id="e5808f9e-ff5e-49ea-ba37-2c9c6605adc1" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="15cdb38e-c81b-4dcf-8402-de9f433e2c42" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.51.09.png"><img style="width:1166px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.51.09.png"/></a></figure><p id="66ab2940-c957-40ee-a2d3-da69fa7553bf" class="">
</p></details></li></ul><ul id="33b7bcd9-6e7e-409a-b0dc-aeb58f4d0465" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="7b213ce3-ef7c-4929-89b1-97fd9737d548" class="">With this data set we seem to not need very deep trees, with depth &gt; 3 seeing little changes. The models needing the deepest trees appear to be the base Decision Tree (both test and train), and Random Forest (test). The data set is too small to really test this I suspect.</p></details></li></ul></details></li></ul><ul id="6dbcd631-04b1-4938-b20a-f7538b57a2b7" class="toggle"><li><details><summary>Test 2 - Cars (all except RF)</summary><p id="bb8bd070-dee8-4239-9758-bbd8660f0f2f" class="">(note, RF was removed since it was only as good as bagging in this case)</p><ul id="2987423b-dc46-4091-81be-cd5e24748fa0" class="toggle"><li><details><summary><strong>Config: ./config/x_model/max_depth/test2.ini</strong></summary><figure id="6867ba45-d1b8-43f9-b3d1-80ce517f7838" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.38.00.png"><img style="width:505px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.38.00.png"/></a></figure></details></li></ul><ul id="87ef9c93-82b1-47e5-b7ab-a4c6ffab0093" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="958b607a-2824-440a-83d9-04cd7c3d0c0e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.51.34.png"><img style="width:1165px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.51.34.png"/></a></figure><p id="d4373661-b0f2-4e9d-bdfb-b0729490abbe" class="">
</p></details></li></ul><ul id="0e20a170-924a-44c3-8a0d-4af8adc8a38a" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="7e7a21f3-72b6-42cb-a894-742323fe306d" class="">Boosting stands out here, peaking after only about depth=4, whereas the others are on a more permanent trajectory improving with more depth. Interestingly for AdaBoost it is traditional to use a depth of 1 (stumps), but deeper (but still shallow) trees seem to work fine as well.</p></details></li></ul></details></li></ul><ul id="ce3dbe5f-d0a0-474f-9030-e6eea15e421b" class="toggle"><li><details><summary>Test 3 - Housing (all models)</summary><ul id="117ee679-b203-47c4-bb76-687a55309db7" class="toggle"><li><details><summary><strong>Config: ./config/x_model/max_depth/test3.ini</strong></summary><figure id="5aa7c3a3-ccfb-415e-9f9c-5b77cdfde062" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.38.29.png"><img style="width:484px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.38.29.png"/></a></figure></details></li></ul><ul id="4c27d6b0-bf19-4743-8d43-3120ae2dd048" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="871d4c7b-ba56-48a3-9a69-a64c5063a28e" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.51.56.png"><img style="width:1163px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.51.56.png"/></a></figure><p id="acae25f4-37e6-4f74-acfc-3018f5ee45b3" class="">
</p></details></li></ul><ul id="6e751163-b545-4345-b6cf-a21ea7e00dda" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="2643c2a4-8eaf-4f89-a5ef-c957edcee416" class="">The baseline Decision Tree again needing deeper than other models to get best results. Again boosting did not need as deep a tree as other models, including RF or Bagging. </p></details></li></ul></details></li></ul><ul id="d76defdb-094e-48ce-984a-dabe66bb4487" class="toggle"><li><details><summary>Test 4 - Concrete (all except RF)</summary><p id="87eaf000-36fc-4e2e-bb13-f7aca8de5403" class="">(note, RF was removed since it was only as good as bagging in this case)</p><ul id="f7f81126-463b-4614-94c1-53dd93554e1d" class="toggle"><li><details><summary><strong>Config: ./config/x_model/max_depth/test4.ini</strong></summary><figure id="e12c03f7-1a20-449f-a484-df5f255939b7" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.38.45.png"><img style="width:499px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.38.45.png"/></a></figure></details></li></ul><ul id="f19f6257-5f45-468e-a064-1179ff051ec5" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="306328d8-38c7-4849-839d-3652250f835f" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.52.19.png"><img style="width:1169px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.52.19.png"/></a></figure><p id="0f33a1d5-c65c-4088-9edf-c5b40e019a7e" class="">
</p></details></li></ul><ul id="a6a6c15a-5202-44df-8911-4f373fa822e8" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="a9b9d648-9bcb-4bd2-bb04-78891e9b80b3" class="">Once again, boosting needs less deep trees to do well here, and the basic decision tree needs the deepest.</p></details></li></ul></details></li></ul><ul id="c3e63280-4582-4945-b7e3-566c9f3da8df" class="toggle"><li><details><summary>Conclusions on impact of max_depth</summary><ul id="6d24b671-a70f-465c-bee3-e78fc62bab1b" class="bulleted-list"><li>boosting needs less deep trees (test2, and test 4 show this a lot), DT needs more depth (test 1)</li></ul></details></li></ul><h2 id="c0662cf9-4d63-454c-95b8-26d5c578019d" class="">6.3 how does &#x27;min_split&#x27; impact model performance?</h2><ul id="40bb853a-00ec-4234-afa0-8c7db357afbf" class="toggle"><li><details><summary>Test 1 - Wine (all models)</summary><ul id="c0ad72d8-d6b1-407e-887d-c497b86d5f88" class="toggle"><li><details><summary><strong>Config: ./config/x_model/min_split/test1.ini</strong></summary><figure id="8998e4ce-21e6-4a4c-9467-cd8e92b2df21" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.39.09.png"><img style="width:495px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.39.09.png"/></a></figure></details></li></ul><ul id="96659364-fc12-4aaf-b902-d4af6743feff" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="57e4fad8-b6b7-4603-bd87-89dbd98d3c90" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.52.43.png"><img style="width:1164px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.52.43.png"/></a></figure><p id="73fe05a4-a0e4-4050-82ef-ef6a6d6c2f21" class="">
</p></details></li></ul><ul id="078632aa-4775-4a03-b4e3-28d0b8d8576d" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="b5ede4d6-2538-4c0f-a95b-5282d6673404" class="">Not very conclusive with such a small data set, although the general picture here is that larger min_split does degrade results for all models (although boost_train shows none of this at these levels.) The other aspect that this graph sows is that the impact is much more on test results than training results. </p></details></li></ul></details></li></ul><ul id="cff76e67-2309-4c8a-8d7b-321d208660e0" class="toggle"><li><details><summary>Test 2 - Cars (all except RF)</summary><p id="74cd739e-8f0a-43be-83c4-ced19dcbe7cd" class="">(note, RF was removed since it was only as good as bagging in this case)</p><ul id="796595c4-7327-4e9f-9aa9-ecbb3e63766c" class="toggle"><li><details><summary><strong>Config: ./config/x_model/min_split/test2.ini</strong></summary><figure id="c65efcd0-bd09-469a-bc6f-343cb28041f1" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.39.37.png"><img style="width:495px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.39.37.png"/></a></figure></details></li></ul><ul id="e370fa19-3077-4a7e-8017-49daf89e57c6" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="b370fca0-c008-41fe-a3a6-f9e4db8b7df4" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.53.09.png"><img style="width:1171px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.53.09.png"/></a></figure><p id="21c1a332-2408-4432-9444-057f1dd26014" class="">
</p></details></li></ul><ul id="ced2ceea-91ce-4966-8227-7fa64ed29c33" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="2a4efad7-4969-4a11-ac8d-4c0997570f9d" class="">Generally having a smaller min_split allows the model to learn better and get better results for both training and test. The exception here is boosting which seems to be far more robust as far as min split is concerned, and still gets similar results even with quite large min_split values.</p></details></li></ul></details></li></ul><ul id="b0a0d5be-7571-4968-898b-b6cfaf02185e" class="toggle"><li><details><summary>Test 3 - Housing (all models)</summary><ul id="b7cc84ca-7d11-443f-910a-62c005ede621" class="toggle"><li><details><summary><strong>Config: ./config/x_model/min_split/test3.ini</strong></summary><figure id="e203cb37-e8bc-4578-aa76-16634fc0e598" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.39.57.png"><img style="width:486px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.39.57.png"/></a></figure></details></li></ul><ul id="20932434-62e2-4734-8fed-e9032cebb00c" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="3283b340-3e07-4806-8b3d-8ae02cab5487" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.53.35.png"><img style="width:1165px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.53.35.png"/></a></figure><p id="507fe24c-21ff-4073-b44a-b927f2c21260" class="">
</p></details></li></ul><ul id="513c5799-64ac-4e25-8da9-fca76e9da721" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="5b72755e-b004-4222-b46e-2f6163030864" class="">Again we mostly see diminished scores for higher min_splits, with exception of boosting, which seems to actually do better (on test set) with larger min_split- although note the scale or the score is actually very shallow here os it&#x27;s not much of a change really.) It is to be expected that the training score will come down with a bigger min_split as the model is less likely to overfit. This is more pronounced for the Decision Tree than the other models. For test results, again more pronounced for Decision Tree, with others being largely flat (except boosting).</p><p id="a30a24dd-545a-418b-97b2-9e79b7952947" class="">
</p></details></li></ul></details></li></ul><ul id="f5124f75-b602-407b-9e1f-cf310cedf819" class="toggle"><li><details><summary>Test 4 - Concrete (all except RF)</summary><p id="13b8ddf8-8174-471a-89b8-214c4a9adb3b" class="">(note, RF was removed since it was only as good as bagging in this case)</p><ul id="4db26ef4-3d26-40eb-b68f-796d76eb0319" class="toggle"><li><details><summary><strong>Config: ./config/x_model/min_split/test4.ini</strong></summary><figure id="e6fcfadb-b273-49c2-969b-09c271a110f0" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.40.14.png"><img style="width:502px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.40.14.png"/></a></figure></details></li></ul><ul id="2b21209b-6ce0-48b6-be07-d7d5973ebd1a" class="toggle"><li><details><summary><strong>Results</strong></summary><figure id="577b9a0c-d9fe-42f0-9a30-fa3b261e99b9" class="image"><a href="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.54.14.png"><img style="width:1172px" src="Experiment%20Results%2042655887eab24a72a41faf613de24524/Screenshot_2021-09-01_at_17.54.14.png"/></a></figure><p id="278844bc-f397-4f2e-825f-259abafc2093" class="">
</p></details></li></ul><ul id="b235937f-312f-4474-94c1-7fb2da3a9aba" class="toggle"><li><details><summary><strong>Thoughts</strong></summary><p id="593bce11-c98a-4041-857f-4e86e97e5226" class="">Similar results in that generally larger min_split gets us smaller scores, but this is not the case with boosting that seems to cope better with a larger min_split (which will mean more shallow trees, which seem to suit boosting!)</p></details></li></ul></details></li></ul><ul id="776e6e91-7efc-404d-9148-7d8870d8feef" class="toggle"><li><details><summary>Conclusions on impact of min_split</summary><ul id="d65aed62-daab-4fd1-a3ba-c0a0ba51a283" class="bulleted-list"><li>Overall bigger min_split is lower score, particularly for training (less closely fitting to data)</li></ul><ul id="dc35336e-d390-4072-b8ee-40352f9f578d" class="bulleted-list"><li>This is more pronounced for the simpler Decision Tree that has a problem with overfitting</li></ul><ul id="be468890-2760-4706-8711-cee4ec7be781" class="bulleted-list"><li>This is least so with boosting - since a high min_split in effect does same job as limit the depth, this is not surprising as we saw boosting doing well with shorter/less deep trees too.</li></ul><ul id="5cfa6d60-f414-43b2-b96c-fcca893ff06f" class="bulleted-list"><li>note : test 4 is very clear on boosting doing much better with larger min_split than others</li></ul></details></li></ul><p id="f0a25596-a1dc-476c-94af-8dea7294ebdb" class="">
</p><p id="1a731378-503e-450e-87a3-dab58fd00067" class="">
</p><p id="da9b144e-abe5-4f59-a334-c3066d31f75d" class="">
</p><p id="8044224a-c7eb-4dc5-8788-e0f1ad16fa4b" class="">
</p><p id="ce3f8eca-7ac1-440d-b244-b98dc72dbf49" class="">
</p><p id="b85e674f-a4c3-4710-a5ca-2cd366eb4e06" class="">
</p><p id="304296da-d400-45ce-bb5c-4295246d4149" class="">
</p><p id="c842a319-cec0-4b58-9e03-a05d41492857" class="">
</p><p id="bd915e3b-2f45-46ef-a50d-871fdd97dfef" class="">
</p><p id="5992bfc1-15e6-46cf-92a5-c484ca9a007c" class="">
</p><p id="c6c15cdf-2d21-4a1f-9c37-4203ccf2f6fd" class="">
</p><p id="6b73b387-9d9d-4392-b46a-2c8a40fc2307" class="">
</p><p id="3e5da860-5f5e-4868-91e7-b13b73c71e82" class="">
</p></div></article></body></html>